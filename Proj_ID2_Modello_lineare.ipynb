{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XAd4JyBYFvgq",
        "nfnIlrMYF09a"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import librerie"
      ],
      "metadata": {
        "id": "R9GeDARYE6SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marika-rago/projectID2ML\n",
        "%cd projectID2ML"
      ],
      "metadata": {
        "id": "W0o1e78XigyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import pprint\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n",
        "from IPython.display import Audio, display\n",
        "from torch import amp\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.signal import get_window"
      ],
      "metadata": {
        "id": "qekV2rAEHe_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", message=\".*TorchCodec.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*torch.hann_window.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"librosa\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"FFMPEG_LOG_LEVEL\"]=\"quiet\""
      ],
      "metadata": {
        "id": "ehLQofA5HjP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Riproducibilità"
      ],
      "metadata": {
        "id": "oct6cEhdHmlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiamo i parametri di configurazione principali e fissiamo un `seed` globale per la riproducibilità. Questo permette di avere risultati coerenti tra le varie esecuzioni.\n",
        "\n",
        "Nel dizionario `CONFIG` ci sono gli iperparametri comuni a tutta la pipeline, tra cui:\n",
        "- **Frequenza di campionamento** (`sample_rate`)\n",
        "- **Modalità di padding** (`pad_mode`): con \"reflect\", le estremità del segnale vengono riflesse, riducendo gli artefatti ai bordi.\n",
        "- **Parametri della STFT** (`n_fft`, `hop_length`, `win_length`, `window`)  \n",
        "- **Parametri di normalizzazione** (`top_db`, `epsilon`)  \n",
        "- **Frazione del dataset** utilizzata per l’esperimento  \n",
        "- **Seed** per la riproducibilità  \n",
        "\n",
        "Alla fine selezioniamo automaticamente il dispositivo di calcolo (GPU se disponibile).\n"
      ],
      "metadata": {
        "id": "xrG4CaU4bZ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Riproducibilità\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed) # seed per random\n",
        "    np.random.seed(seed) # seed per NumPy\n",
        "    torch.manual_seed(seed) # seed per PyTorch (CPU)\n",
        "    torch.cuda.manual_seed_all(seed) # seed per cuda\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Seed globale impostato a {seed}\")"
      ],
      "metadata": {
        "id": "ij4NiAm_Hxfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">**ATTENZIONE!!!**\n",
        ">\n",
        ">All'interno del dizionario CONFIG è presente la voce `skip_training`. Se si >vuole eseguire il training cambiare la voce in `False` (di base è settata a `True`).\n",
        "\n"
      ],
      "metadata": {
        "id": "8GuOFwmCMI-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"sample_rate\": 32000,     # Frequenza di campionamento audio\n",
        "    \"pad_mode\": \"reflect\",    # Modalità di padding\n",
        "    \"n_fft\": 1024,            # Dimensione della FFT\n",
        "    \"hop_length\": 256,        # Passo tra due finestre consecutive\n",
        "    \"win_length\": 1024,       # Lunghezza effettiva della finestra di analisi\n",
        "    \"window\": \"hann\",         # Tipo di finestra (Hann per ridurre le discontinuità)\n",
        "    \"center\": True,           # Centra ogni frame rispetto al segnale originale\n",
        "    \"top_db\": 80.0,           # Range dinamico per la conversione in dB\n",
        "    \"epsilon\": 1e-8,          # Termine di sicurezza per evitare divisioni per zero\n",
        "    \"seed\": 42,               # Seed di riproducibilità\n",
        "    \"dataset_fraction\": 0.16, # Percentuale del dataset usata\n",
        "    \"skip_training\": True,    # Salto del training loop se è true\n",
        "}\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])"
      ],
      "metadata": {
        "id": "APAGw6dcymyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "WPnn4ousHzek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "PG09OlQZFDUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per il training e la valutazione del mio modello è stato utilizzato il dataset **FMA**, in particolare la versiona *fma_small*, che contiene $8.000$ clip da $30$ secondi ciascuna in formati $.mp3$.\n",
        "\n",
        "Quello che facciamo nelle seguenti celle è:\n",
        "\n",
        "1.   scaricare e decomprimere il dataset FMA;\n",
        "2.   contare il numero di file $.mp3$ per capire quanti utilizzarne;\n",
        "3.   selezionare casualmente una porzione del dataset, definita con il parametro `CONFIG[\"dataset_fraction\"]`;\n",
        "4.   dividiamo i file selezionati in tre sottoinsimi, train_files, val_files e test_files;\n",
        "5.   salviamo sul disco la lista dei file selezionati.   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jA4AOXfffAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
        "!unzip fma_small.zip"
      ],
      "metadata": {
        "id": "_Ox5rl7IH8p0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/fma_small\"  # cartella che contiene il dataset compresso\n",
        "\n",
        "# Conta tutti i file .mp3 nelle sottocartelle\n",
        "count = sum(len([f for f in files if f.endswith(\".mp3\")]) for _, _, files in os.walk(root_dir))\n",
        "\n",
        "print(f\"Numero totale di file MP3: {count}\")"
      ],
      "metadata": {
        "id": "MIBH9xmmH9yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = CONFIG[\"seed\"]\n",
        "fraction = CONFIG[\"dataset_fraction\"]\n",
        "\n",
        "# Imposta il seed per la riproducibilità\n",
        "random.seed(seed)\n",
        "\n",
        "selected_files = []\n",
        "\n",
        "# Scorri tutte le sottocartelle e seleziona un tot di file per ciascuna\n",
        "for subdir, _, files in os.walk(root_dir):\n",
        "    mp3_files = [os.path.join(subdir, f) for f in files if f.endswith(\".mp3\")]\n",
        "    if not mp3_files: # Salta le cartelle vuote\n",
        "        continue\n",
        "    n_select = max(1, int(len(mp3_files) * fraction))\n",
        "    chosen = random.sample(mp3_files, n_select)\n",
        "    selected_files.extend(chosen)\n",
        "\n",
        "print(f\"Totale file selezionati: {len(selected_files)} ({fraction*100:.0f}% del dataset)\")\n",
        "\n",
        "# Salva la lista su file\n",
        "with open(\"selected_files.txt\", \"w\") as f:\n",
        "    for path in selected_files:\n",
        "        f.write(path + \"\\n\")\n",
        "\n",
        "print(\"Lista salvata in 'selected_files.txt'\")"
      ],
      "metadata": {
        "id": "qxJiSRcjIAY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mischia la lista in modo riproducibile\n",
        "random.seed(42)\n",
        "random.shuffle(selected_files)\n",
        "\n",
        "# Suddivisione (907 + 150 + 150 = 1207)\n",
        "train_files = selected_files[:907]\n",
        "val_files = selected_files[907:1057]\n",
        "test_files = selected_files[1057:]\n",
        "\n",
        "print(f\"Train files: {len(train_files)}\")\n",
        "print(f\"Val files: {len(val_files)}\")\n",
        "print(f\"Test files: {len(test_files)}\")\n"
      ],
      "metadata": {
        "id": "hDVICPxoIDpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Degradazioni"
      ],
      "metadata": {
        "id": "uCeMUBHYFGmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `AudioDegradationDataset` va a definire il dataset che poi utilizziamo per addestrare il modello.\n",
        "L'obiettivo di questa classe è simulare vari tipi di degradazioni audio realistiche a partire dalle clip pulite del datset FMA. In questo modo avremo coppie *(degraded, clean)* su cui il modello deve imparare a ricostruire l'audio eliminando gli artefatti.\n",
        "\n",
        "Le degradazioni vengono applicate dinamicamente ad ogni clip, aumentando così la variabilità dei dati.\n",
        "\n",
        "---\n",
        "\n",
        "**Struttura della Classe**\n",
        "\n",
        "\n",
        "1. *Caricamento e segmentazione* :\n",
        "  ogni clip viene caricata a $32 kHz$ e ridotta a $3$ secondi, in modo da avere input di lunghezza breve e non appesantire il training. Se la clip è più corta da $3$ secondi viene riempita con zeri (padded). Inoltre i segmenti vengono normalizzati.\n",
        "\n",
        "2. *identity_prob* :\n",
        "  con una probabilità del $15\\%$ la clip non viene degradata e rimane uguale a clean. Questo serve per far capire al modello che non deve alterare segnali che sono già buoni di partenza.\n",
        "\n",
        "3. *Degradazioni* :\n",
        "  il metodo `degradation` applica casualmente una delle degradazioni specificate in `degradation_types`. Le varie degradazioni che possono essere applicate sono:\n",
        "\n",
        "      * **Quantizzazione (`quantize`)**: simula la riduzione delle profondità di bit, come avviene nei formati compressi. L'audio viene scalato e arrotondato a una risoluzione di 6, 8 o 10 bit, viene inoltre aggiunto un piccolo `dither` per evitare quantizzazione troppo regolare.\n",
        "\n",
        "      * **Low-pass (`lowpass`)**: simula la perdita delle alte frequenze, l'effetto è che il suono risulta più \"ovattato\". Applica un filtro passa-basso con frequenza di taglio casuale tra $2.5$ e $7$  $kHz$.\n",
        "\n",
        "      * **Clipping (`clipping`)**: simila la distorsione da saturazione del segnale. Vengono troncati tutti i valori oltre una soglia casuale tra $0.6$ e $0.9$.\n",
        "\n",
        "      * **Rumore (`noise`)**: simula un rumore di fondo bianco o rosa, l'effetto è un \"fruscio\" costante.\n",
        "\n",
        "      * **Reverbero (`reverb`)**: simula l'effetto di una stanza. Viene creato una specie di eco con decadimento esponenziale.\n",
        "\n",
        "      * **Distorsione armonica (`distort`)**: simula l'effetto di saturazione dei dispositivi analogici. Applica una non linearita, `tanh`.\n",
        "\n",
        "      * **Tonal Stripes (`tonal_stripes`)**: simula inferenza sinusoidali periodiche. Genera alcune sinusoidi a frequenze casuali (tra $200$ e $800$ $Hz$) e le somma al segnale con ampiezza limitata.\n",
        "\n",
        "4. *Conversione in spettrogrammi lineari* :  \n",
        "  Dopo la degradazione, entrambi i segnali (pulito e degradato) vengono trasformati tramite STFT in spettrogrammi lineari di ampiezza.\n",
        "  Le magnitudini vengono convertite in decibel (dB) e poi normalizzate tra 0 e 1. I risultati vengono infine convertiti in tensori PyTorch, pronti per essere utilizzati nel modello.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J2y4sgsTk9jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDegradationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, audio_files, sample_rate=32000, segment_length=2.0,\n",
        "                 degradation_types=['quantize', 'lowpass', 'noise', 'tonal_stripes'],\n",
        "                 identity_prob=0.15,\n",
        "                 deterministic=False,\n",
        "                 seed=42):\n",
        "        self.audio_files = audio_files\n",
        "        self.sr = sample_rate\n",
        "        self.segment_samples = int(segment_length * sample_rate)\n",
        "        self.degradation_types = degradation_types\n",
        "        self.identity_prob = identity_prob\n",
        "        self.deterministic = deterministic # Se True blocca la randomizzazione\n",
        "        self.seed = seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "\n",
        "    # Tonal stripes\n",
        "    def add_tonal_stripes(self, audio, sr, num_tones=3, amp_range=(0.02, 0.05),\n",
        "                          min_freq=200, max_freq=8000):\n",
        "        n = len(audio)\n",
        "        t = np.linspace(0, n / sr, n, endpoint=False)\n",
        "        audio = audio.copy()\n",
        "\n",
        "        for _ in range(random.randint(1, num_tones)):\n",
        "            freq = random.uniform(min_freq, max_freq)\n",
        "            amp = random.uniform(*amp_range)\n",
        "            stripe = np.sin(2 * np.pi * freq * t)\n",
        "\n",
        "            # Applica dissolvenze casuali\n",
        "            if random.random() < 0.5:\n",
        "                fade_len = random.randint(int(0.2*n), int(0.6*n))\n",
        "                start = random.randint(0, n - fade_len)\n",
        "                fade = np.linspace(0, 1, fade_len)\n",
        "                stripe[start:start+fade_len] *= fade[::-1] if random.random() < 0.5 else fade\n",
        "\n",
        "            audio += amp * stripe\n",
        "\n",
        "        # Evita saturazione\n",
        "        return np.clip(audio, -1.0, 1.0)\n",
        "\n",
        "\n",
        "    # Degradazione\n",
        "    def degradation(self, audio):\n",
        "        x = audio.copy()\n",
        "\n",
        "        # Scelta casuale\n",
        "        degradation = random.choice(self.degradation_types)\n",
        "\n",
        "        # Quantizzazione\n",
        "        if degradation == 'quantize':\n",
        "            bits = np.random.choice([6, 8, 10])\n",
        "            q = 2 ** bits\n",
        "            dither = np.random.uniform(-1/q, 1/q, size=x.shape)\n",
        "            x = np.round(x * q) / q + dither\n",
        "\n",
        "        # Low-pass\n",
        "        elif degradation == 'lowpass':\n",
        "            cutoff = float(np.random.uniform(2500, 7000))\n",
        "            xt = torch.from_numpy(x).unsqueeze(0)\n",
        "            xt = torchaudio.functional.lowpass_biquad(xt, self.sr, cutoff)\n",
        "            x = xt.squeeze(0).numpy()\n",
        "\n",
        "        # Clipping\n",
        "        elif degradation == 'clipping':\n",
        "            thr = float(np.random.uniform(0.6, 0.9))\n",
        "            x = np.clip(x, -thr, thr)\n",
        "\n",
        "        # Rumore bianco o rosa\n",
        "        elif degradation == 'noise':\n",
        "            if np.random.rand() < 0.5:\n",
        "                std = np.random.uniform(0.01, 0.05)\n",
        "                x += np.random.randn(len(x)) * std\n",
        "            else:\n",
        "                white = np.random.randn(len(x))\n",
        "                b = np.cumsum(white)\n",
        "                pink = b / np.max(np.abs(b))\n",
        "                std = np.random.uniform(0.01, 0.04)\n",
        "                x += pink * std\n",
        "\n",
        "        # Reverbero\n",
        "        elif degradation == 'reverb':\n",
        "            decay = np.random.uniform(0.3, 0.9)\n",
        "            ir_len = np.random.randint(2000, 6000)\n",
        "            ir = np.exp(-np.linspace(0, decay, ir_len))\n",
        "            x = np.convolve(x, ir, mode='same')\n",
        "            x = x / (np.max(np.abs(x)) + 1e-8)\n",
        "\n",
        "        # Distorsione\n",
        "        elif degradation == 'distort':\n",
        "            gain = np.random.uniform(1.5, 3.0)\n",
        "            x = np.tanh(x * gain)\n",
        "\n",
        "        # Tonal stripes\n",
        "        elif degradation == 'tonal_stripes':\n",
        "            x = self.add_tonal_stripes(x, self.sr)\n",
        "\n",
        "        return np.clip(x, -1.0, 1.0)\n",
        "\n",
        "\n",
        "    # Caricamento di un file audio\n",
        "    def file_load(self, path):\n",
        "        try:\n",
        "            audio, _ = librosa.load(path, sr=self.sr, mono=True)\n",
        "            return audio\n",
        "\n",
        "        except Exception:\n",
        "            from pydub import AudioSegment\n",
        "            audio = AudioSegment.from_file(path)\n",
        "            samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
        "            samples = samples / (np.max(np.abs(samples)) + 1e-8)\n",
        "            return samples\n",
        "\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Se deterministic=True imposta un seed fisso\n",
        "        if self.deterministic:\n",
        "            np.random.seed(self.seed + idx)\n",
        "            random.seed(self.seed + idx)\n",
        "\n",
        "        path = self.audio_files[idx]\n",
        "\n",
        "        # Gestisce eventuali errori nel caricamento del file audio\n",
        "        try:\n",
        "            audio = self.file_load(path)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Errore nel file {path}: {e}\")\n",
        "            new_idx = (idx + 1) % len(self.audio_files)\n",
        "            return self.__getitem__(new_idx)\n",
        "\n",
        "        # Segmentazione\n",
        "        if len(audio) > self.segment_samples:\n",
        "            start = np.random.randint(0, len(audio) - self.segment_samples)\n",
        "            audio = audio[start:start + self.segment_samples]\n",
        "        else:\n",
        "            audio = np.pad(audio, (0, self.segment_samples - len(audio)))\n",
        "\n",
        "        # Normalizzazione\n",
        "        max_val = np.max(np.abs(audio)) + CONFIG[\"epsilon\"]\n",
        "        audio = audio / max_val\n",
        "\n",
        "        # Identità stocastica\n",
        "        if np.random.rand() < self.identity_prob:\n",
        "            degraded = audio.copy()\n",
        "        else:\n",
        "            degraded = self.degradation(audio)\n",
        "\n",
        "        # STFT\n",
        "        win = scipy.signal.get_window(CONFIG[\"window\"], CONFIG[\"win_length\"], fftbins=True)\n",
        "        S_clean = librosa.stft(audio, n_fft=CONFIG[\"n_fft\"], hop_length=CONFIG[\"hop_length\"],\n",
        "                               win_length=CONFIG[\"win_length\"], window=win,\n",
        "                               center=CONFIG[\"center\"], pad_mode=CONFIG[\"pad_mode\"])\n",
        "        S_deg = librosa.stft(degraded, n_fft=CONFIG[\"n_fft\"], hop_length=CONFIG[\"hop_length\"],\n",
        "                             win_length=CONFIG[\"win_length\"], window=win,\n",
        "                             center=CONFIG[\"center\"], pad_mode=CONFIG[\"pad_mode\"])\n",
        "\n",
        "        # Magnitudine e conversione in dB\n",
        "        Mag_clean = np.abs(S_clean)\n",
        "        Mag_deg = np.abs(S_deg)\n",
        "\n",
        "        S_db_clean = librosa.amplitude_to_db(np.maximum(Mag_clean, CONFIG[\"epsilon\"]),\n",
        "                                             ref=np.max, top_db=CONFIG[\"top_db\"])\n",
        "        S_db_deg = librosa.amplitude_to_db(np.maximum(Mag_deg, CONFIG[\"epsilon\"]),\n",
        "                                           ref=np.max, top_db=CONFIG[\"top_db\"])\n",
        "\n",
        "        # Normalizzazione su [0, 1]\n",
        "        Mag_clean_01 = np.clip((S_db_clean + CONFIG[\"top_db\"]) / CONFIG[\"top_db\"], 0.0, 1.0)\n",
        "        Mag_deg_01 = np.clip((S_db_deg + CONFIG[\"top_db\"]) / CONFIG[\"top_db\"], 0.0, 1.0)\n",
        "\n",
        "        # Conversione in tensori PyTorch\n",
        "        clean_spec = torch.from_numpy(Mag_clean_01).unsqueeze(0).float()\n",
        "        degraded_spec = torch.from_numpy(Mag_deg_01).unsqueeze(0).float()\n",
        "\n",
        "        return degraded_spec, clean_spec\n"
      ],
      "metadata": {
        "id": "1yLvBfQ2bzCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gruppi di degradazioni\n",
        "gruppo_A = ['quantize', 'tonalstripes', 'noise']\n",
        "gruppo_B = ['clipping', 'reverb', 'lowpass', 'distort']"
      ],
      "metadata": {
        "id": "FrG3ONEbISWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Gruppo A"
      ],
      "metadata": {
        "id": "Rd2QcIH-FOWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Richiamiamo la classe AudioDegradationDataset sulle degradazioni del gruppo A\n",
        "train_dataset_A = AudioDegradationDataset(\n",
        "    train_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=False\n",
        ")\n",
        "val_dataset_A = AudioDegradationDataset(\n",
        "    val_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=True, seed=23\n",
        ")\n",
        "test_dataset_A = AudioDegradationDataset(\n",
        "    test_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=True,\n",
        "    seed=19\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset_A)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_A)}\")\n",
        "print(f\"Test samples: {len(test_dataset_A)}\")\n"
      ],
      "metadata": {
        "id": "VAfXbmAgIWmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Gruppo B"
      ],
      "metadata": {
        "id": "w9xx-nNyFUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Richiamiamo la classe AudioDegradationDataset sulle degradazioni del gruppo B\n",
        "train_dataset_B = AudioDegradationDataset(\n",
        "    train_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=False\n",
        ")\n",
        "val_dataset_B = AudioDegradationDataset(\n",
        "    val_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=True,\n",
        "    seed=23\n",
        ")\n",
        "test_dataset_B = AudioDegradationDataset(\n",
        "    test_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=True,\n",
        "    seed=19\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset_B)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_B)}\")\n",
        "print(f\"Test samples: {len(test_dataset_B)}\")\n"
      ],
      "metadata": {
        "id": "RPyc-E_fIaFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizzazione esempio"
      ],
      "metadata": {
        "id": "tkF8DPWmIwu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funzione `show_example_spectrogram` visualizziamo la rappresentazione spettrale (STFT lineare) dei segnali clean e degraded.\n",
        "Gli spettrogrammi sono visualizzati in scala di intensità normalizzata tra\n",
        "$[0, 1]$ ottenuta dalla conversione in decibel (dB).\n",
        "\n",
        "Sull'asse delle $x$ abbiamo il tempo (frame), sull'asse delle $y$ abbiamo la frequenza.\n",
        "\n",
        "---\n",
        "\n",
        "Con la funzione `reconstruct_audio` ricostruiamo un segnale audio (che possiamo sentire) a partire dallo spettrogramma di magnitudine.\n",
        "Poiché la STFT utilizzata nel dataset non conserva la fase, usiamo l’algoritmo di Griffin-Lim, che stima iterativamente una fase coerente a partire dalla magnitudine.\n",
        "\n",
        "Questa fase permette di valutare in modo percettivo la qualità delle degradazioni."
      ],
      "metadata": {
        "id": "xuopHWH85R8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example_spectrogram(dataset, idx=0):\n",
        "\n",
        "    # Estrae la coppia (degraded, clean)\n",
        "    degraded_sp, clean_sp = dataset[idx]\n",
        "\n",
        "    # Rimuove la dimensione del canale\n",
        "    clean_sp = clean_sp.squeeze().numpy()\n",
        "    degraded_sp = degraded_sp.squeeze().numpy()\n",
        "\n",
        "    print(f\"Shape clean_spec: {clean_sp.shape} | Shape degraded_spec: {degraded_sp.shape}\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    # Plot clean\n",
        "    im1 = axes[0].imshow(clean_sp, aspect='auto', origin='lower')\n",
        "    axes[0].set_title('Clean Linear Spectrogram (dB --> [0,1])'); fig.colorbar(im1, ax=axes[0])\n",
        "\n",
        "    # Plot degraded\n",
        "    im2 = axes[1].imshow(degraded_sp, aspect='auto', origin='lower')\n",
        "    axes[1].set_title('Degraded Linear Spectrogram (dB --> [0,1])'); fig.colorbar(im2, ax=axes[1])\n",
        "\n",
        "    plt.tight_layout();\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vmsQjCG4I6gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_audio(dataset, idx=0, n_iter=32):\n",
        "\n",
        "    # Estrae la coppia (degraded, clean)\n",
        "    degraded_spec, clean_spec = dataset[idx]\n",
        "\n",
        "    # Converti i tensori in numpy [F, T]\n",
        "    degraded_spec = degraded_spec.squeeze(0).numpy()\n",
        "    clean_spec = clean_spec.squeeze(0).numpy()\n",
        "\n",
        "    # Da [0,1] --> dB --> ampiezza\n",
        "    S_db_clean = clean_spec * CONFIG[\"top_db\"] - CONFIG[\"top_db\"]\n",
        "    S_db_degraded = degraded_spec * CONFIG[\"top_db\"] - CONFIG[\"top_db\"]\n",
        "\n",
        "    Mag_clean = librosa.db_to_amplitude(S_db_clean)\n",
        "    Mag_degraded = librosa.db_to_amplitude(S_db_degraded)\n",
        "\n",
        "    # Griffin-Lim per ricostruire fase e segnale\n",
        "    win = scipy.signal.get_window(CONFIG[\"window\"], CONFIG[\"win_length\"], fftbins=True)\n",
        "\n",
        "    y_clean = librosa.griffinlim(\n",
        "        Mag_clean,\n",
        "        n_iter=n_iter,\n",
        "        hop_length=CONFIG[\"hop_length\"],\n",
        "        win_length=CONFIG[\"win_length\"],\n",
        "        window=win,\n",
        "        center=CONFIG[\"center\"]\n",
        "    )\n",
        "\n",
        "    y_degraded = librosa.griffinlim(\n",
        "        Mag_degraded,\n",
        "        n_iter=n_iter,\n",
        "        hop_length=CONFIG[\"hop_length\"],\n",
        "        win_length=CONFIG[\"win_length\"],\n",
        "        window=win,\n",
        "        center=CONFIG[\"center\"]\n",
        "    )\n",
        "\n",
        "    # Normalizza\n",
        "    y_clean = y_clean / (np.max(np.abs(y_clean)) + CONFIG[\"epsilon\"])\n",
        "    y_degraded = y_degraded / (np.max(np.abs(y_degraded)) + CONFIG[\"epsilon\"])\n",
        "\n",
        "    # Ascolto\n",
        "    print(f\"Clean ricostruito con Griffin-Lim ({n_iter} iterazioni):\")\n",
        "    display(Audio(y_clean, rate=CONFIG[\"sample_rate\"]))\n",
        "\n",
        "    print(f\"Degraded ricostruito con Griffin-Lim ({n_iter} iterazioni):\")\n",
        "    display(Audio(y_degraded, rate=CONFIG[\"sample_rate\"]))\n",
        "\n",
        "    # Per confronto, ascolta anche la versione originale dal dataset\n",
        "    path = dataset.audio_files[idx]\n",
        "    original_audio = dataset.file_load(path)\n",
        "    max_val = np.max(np.abs(original_audio)) + CONFIG[\"epsilon\"]\n",
        "    original_audio = original_audio / max_val\n",
        "    print(\"Audio originale (caricato da file):\")\n",
        "    display(Audio(original_audio, rate=CONFIG[\"sample_rate\"]))\n"
      ],
      "metadata": {
        "id": "_BnYOYZ7JRqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio Gruppo A"
      ],
      "metadata": {
        "id": "Q839DWm6J5dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 4\n",
        "show_example_spectrogram(train_dataset_A, idx=idx)\n",
        "reconstruct_audio(train_dataset_A, idx=idx, n_iter=64)"
      ],
      "metadata": {
        "id": "winaSs1nJ8pn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio Gruppo B"
      ],
      "metadata": {
        "id": "Fkh1DQ93J_bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 4\n",
        "show_example_spectrogram(train_dataset_B, idx=idx)\n",
        "reconstruct_audio(train_dataset_B, idx=idx, n_iter=64)"
      ],
      "metadata": {
        "id": "osnJzl7rJ_bN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "OTtVmAFpFZcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopo la definizione dei dataset per i gruppi $A$ e $B$, definiamo ora i corrispondenti `DataLoader`, che gestiscono il caricamento e la preparazione dei batch per il training e la validazione.\n",
        "\n",
        "---\n",
        "\n",
        "Settiamo i parametri generali con `LOADER_CONFIG`:\n",
        "\n",
        "* `batch_size` = $16$, è il numero di esempi per batch.\n",
        "\n",
        "* `num_workers` = $0$, lo impostiamo a $0$ per compatibilità con Colab evitando di saturare la RAM.\n",
        "\n",
        "* `pin_memory` = $True$, blocca la memoria dei batch in RAM, in modo da trasferirli più velocemente sulla GPU.\n",
        "\n",
        "* `persistent_workers` = $False$, evita che i processi di caricamento restino attivi dopo ogni epoca.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abcDWg26z61t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri generali\n",
        "LOADER_CONFIG = {\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 0,\n",
        "    \"pin_memory\": True,\n",
        "    \"persistent_workers\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "l1gdligjKbb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Gruppo A"
      ],
      "metadata": {
        "id": "b6LGOwsIFcoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_A = DataLoader(\n",
        "    train_dataset_A,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=True, # Mescola i campioni ad ogni epoca\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "val_loader_A = DataLoader(\n",
        "    val_dataset_A,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=False, # non mescola in modo da vere una validazione deterministica\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "test_loader_A = DataLoader(\n",
        "    test_dataset_A,\n",
        "    batch_size=1, # analizza un file per volta\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader_A)}, Val batches: {len(val_loader_A)}, Test batches: {len(test_loader_A)}\")"
      ],
      "metadata": {
        "id": "AZXdT6YeKdWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Gruppo B"
      ],
      "metadata": {
        "id": "5EcqrleDFfbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_B = DataLoader(\n",
        "    train_dataset_B,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "val_loader_B = DataLoader(\n",
        "    val_dataset_B,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "test_loader_B = DataLoader(\n",
        "    test_dataset_B,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader_B)}, Val batches: {len(val_loader_B)}, Test batches: {len(test_loader_B)}\")"
      ],
      "metadata": {
        "id": "f7m5fgAeKhEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello"
      ],
      "metadata": {
        "id": "U7QvKBLghTVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il mio modello prende in input uno spettrogramma normalizzato in $[0,1]$ (lineare o Mel, 1 canale) e produce uno spettrogramma \"enhanced\" della stessa forma.\n",
        "Segue lo stile di una U-Net, e integra un contesto multi-scala.\n",
        "\n",
        "---\n",
        "\n",
        "La funzione `crop_to_match` serve per fare combaciare i due tensori che prende in input. Serve perchè le operazioni di downsampling e upsampling possono generare piccole differenze di pixel, e prima di concatenare le skip connections occorre che le dimensioni siano uguali.\n",
        "\n",
        "---\n",
        "\n",
        "La calsse `MultiScaleCNNBlock` applica tre convoluzioni 2d in parallelo con kernel $3 \\times 3$, $5 \\times 5$, $7 \\times 7$. Successivamente fa una fusione con conv $1 \\times 1$ e poi facoltativamente applica ReLU.\n",
        "\n",
        "Tutto questo viene fatto perchè gli artefatti possono essere locali o più larghi.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `ResidualBlock` applica due conv $3 \\times 3$ con BatchNorm e skip residual, e infine applica ReLU. Questo viene fatto per preservare informazioni ed evitare vanishing gradient.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `UpBlock` fa upsampling bilineare seguita da conv $3 \\times 3$, BatchNorm e infine ReLU.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `SpctralEnhancementNet` è composta da tre parti principali: Encoder, Bottleneck e Decoder.\n",
        "\n",
        "\n",
        "L'**Encoder** riduce progressivamente la risoluzione spaziale dello spettrogramma. Ogni blocco MultiScakeCNNBlock estrae feature locali e globali come detto precedentemente. Dopo ogni blocco, una convoluzione con stride $2$ dimezza le simesioni dello spettro, comprimendo l'informazione e ampliando il campo percettivo. Qui la rete impara a vedere il quadro complessivo del segnale.\n",
        "\n",
        "\n",
        "Nella parte centrale, **Bottleneck**, tre blocchi residiali lavorano mantenendo la profondità costante.\n",
        "Questi blocchi permettono di rielaborare le caratteristiche apprese mantenendo l’informazione originale, grazie alle skip connection tra input e output del blocco.\n",
        "Questo meccanismo stabilizza l’addestramento e migliora la capacità della rete di affinare dettagli senza distruggere la struttura armonica del segnale.\n",
        "Viene applicato anche un Dropout2D per ridurre l’overfitting.\n",
        "\n",
        "\n",
        "Il **Decoder** ricostruisce progressivamente la risoluzione originale dello spettrogramma, invertendo il processo di compressione.\n",
        "Ogni livello effettua un upsampling bilineare seguito da una convoluzione $3 \\times 3$ e da una fusione con le feature corrispondenti dell’encoder.\n",
        "Le skip connections collegano i livelli simmetrici dell’encoder e del decoder, in modo che la rete possa recuperare dettagli fini persi durante il downsampling.\n",
        "Dopo la concatenazione, un nuovo MultiScaleCNNBlock elabora le informazioni combinate, permettendo alla rete di fondere dettagli locali e contesto globale.\n",
        "\n",
        "\n",
        "Infine, un’uscita convoluzionale $1 \\times 1$ riduce i canali a uno solo, producendo il mel-spettrogramma enhanced.\n",
        "L’attivazione finale è lineare, il clamp a $[0, 1]$ viene applicato solo durante il training e il test per coerenza con la normalizzazione dei dati.\n",
        "\n"
      ],
      "metadata": {
        "id": "nBDe-xTz26g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_to_match(source, target):\n",
        "\n",
        "    _, _, h, w = source.shape\n",
        "    _, _, h_t, w_t = target.shape\n",
        "\n",
        "    # Se le dimensioni non coincidono, le ritaglia a quella minima\n",
        "    if h != h_t or w != w_t:\n",
        "        h_min = min(h, h_t)\n",
        "        w_min = min(w, w_t)\n",
        "        source = source[:, :, :h_min, :w_min]\n",
        "        target = target[:, :, :h_min, :w_min]\n",
        "    return source, target"
      ],
      "metadata": {
        "id": "fHURdGa4YarH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleCNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, last_relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Suddivide i canali di outout tra i tre rami\n",
        "        base = out_channels // 3\n",
        "        rem = out_channels - 3 * base\n",
        "        ch1, ch2, ch3 = base, base, base + rem  # branch3 assorbe il resto\n",
        "\n",
        "        # Ramo 1 kernel 3x3\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch1, kernel_size=3, padding=1, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Ramo 2 kernel 5x5\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch2, kernel_size=5, padding=2, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Ramo 3 kernel 7x7\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3, kernel_size=7, padding=3, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Dopo concatenazione riduce i canali con con 1x1\n",
        "        fused_in = ch1 + ch2 + ch3\n",
        "        layers = [nn.Conv2d(fused_in, out_channels, kernel_size=1)]\n",
        "        if last_relu:\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        self.fusion = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Calcola le tre rappresentazioni parallele\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "\n",
        "        # Concatena lungo la dimensione dei canali\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "\n",
        "        # Fusione e riduzione dei canali\n",
        "        return self.fusion(out)"
      ],
      "metadata": {
        "id": "QeX_-MqTYeXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect')\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect')\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x # Skip Connection\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + residual # Somma residua\n",
        "        return F.relu(out)"
      ],
      "metadata": {
        "id": "jdUBnJIuYidZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, padding_mode='reflect')\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x) # Upsampling (fa interpolazione bilineare)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cuy7ETr7YmAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralEnhancementNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder --> estrae feature\n",
        "        self.enc1 = MultiScaleCNNBlock(1, 32)\n",
        "        self.down1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        self.enc2 = MultiScaleCNNBlock(32, 64)\n",
        "        self.down2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        self.enc3 = MultiScaleCNNBlock(64, 128)\n",
        "        self.down3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        # Bottleneck --> 3 blocchi residuali\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "        )\n",
        "\n",
        "        # Decoder --> upsampling progressivo e concatenazione con feture encoder\n",
        "        self.up3 = UpBlock(128, 64)\n",
        "        self.dec3 = MultiScaleCNNBlock(64 + 128, 64)\n",
        "\n",
        "        self.up2 = UpBlock(64, 32)\n",
        "        self.dec2 = MultiScaleCNNBlock(32 + 64, 32)\n",
        "\n",
        "        self.up1 = UpBlock(32, 16)\n",
        "        self.dec1 = MultiScaleCNNBlock(16 + 32, 16, last_relu=False)\n",
        "\n",
        "        # Output --> spettrogramma migliorato (un canale)\n",
        "        self.out = nn.Conv2d(16, 1, kernel_size=1)\n",
        "        self.dropout = nn.Dropout2d(p=0.1) # Regolarizzazione nel bottleneck\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e1d = self.down1(e1)\n",
        "        e2 = self.enc2(e1d)\n",
        "        e2d = self.down2(e2)\n",
        "        e3 = self.enc3(e2d)\n",
        "        e3d = self.down3(e3)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.res_blocks(e3d)\n",
        "        b = self.dropout(b)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.up3(b)\n",
        "        d3, e3 = crop_to_match(d3, e3)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2, e2 = crop_to_match(d2, e2)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1, e1 = crop_to_match(d1, e1)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        # Output\n",
        "        enhanced = self.out(d1)\n",
        "        return enhanced"
      ],
      "metadata": {
        "id": "Q93XFiZThXVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "m4_2N48bFkXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La loss che utilizzo nel mio modello\n",
        "`HybridLoss` combina diverse componenti, ognuna della quali si occupa di un aspetto diverso della qualità audio.\n",
        "\n",
        "La loss totale è una somma pesata dei vari termini:\n",
        "\n",
        "$$L_{tot} = w_c \\cdot L_{charb} + w_{hf} \\cdot L_{hf} + w_{dnh} \\cdot L_{dnh} + w_{f} \\cdot L_{flat} + w_{bw} \\cdot L_{bw} + w_e \\cdot L_{energy} $$\n",
        "\n",
        "Dove:\n",
        "\n",
        "* **Charbonnier Loss**: penalizza le differenze punto per punto tra lo spettrogramma predetto ($x$) e il target ($y$). È robusta agli outliers.\n",
        "\n",
        "$$L_{charb} = \\frac{1}{N} \\sum_{f,t}\n",
        "\\sqrt{(x_{f,t} - y_{f,t})^2 + \\varepsilon^2}$$\n",
        "\n",
        "* **High Frequency Loss**: enfatizza le regioni ad alta frequenza e ad alta energia, poichè mi sono accorta che il modello tendeva a smussare nella zona alta.\n",
        "\n",
        "$$L_{hf} = \\frac{1}{N} \\sum_{f,t}\n",
        "w(f) \\cdot m_{f,t} \\, |x_{f,t} - y_{f,t}|$$\n",
        "\n",
        "* **Do No Harm Loss**: scoraggia la rete a modificare le zone già \"buone\".\n",
        "\n",
        "$$L_{dnh} = \\frac{1}{N} \\sum_{f,t}\n",
        "(1 - n_{f,t}) \\, |x_{f,t} - d_{f,t}| $$\n",
        "\n",
        "* **Spectral Flatness e Tonal Penalty Loss**: mantiene la coerenza tonale, evitando che l'output risulti \"metallico\" o troppo \"piatto\".\n",
        "\n",
        "$$SF(S) = \\frac{\\exp \\left( \\frac{1}{T} \\sum_{t} \\log(S_{f,t} + \\varepsilon) \\right)}\n",
        "{\\frac{1}{T} \\sum_{t} (S_{f,t} + \\varepsilon)} $$\n",
        "\n",
        "$$L_{flat} = \\frac{1}{F} \\sum_{f}\n",
        "|SF(x_f) - SF(y_f)| $$\n",
        "\n",
        "* **Band Weighted L1 Loss**: rafforza la coerenza nelle bande di frequenza più importanti per l'udito umano.\n",
        "\n",
        "$$L_{bw} = \\frac{1}{N} \\sum_{f,t}\n",
        "w(f) \\, |x_{f,t} - y_{f,t}| $$\n",
        "\n",
        "* **Energy Consistency Loss**: preserva il volume e il bilanciamento energetico del segnale.\n",
        "\n",
        "$$L_{energy} = \\frac{1}{B} \\sum_{b=1}^{B}\n",
        "\\left| E(x_b) - E(y_b) \\right|\n",
        "\\quad \\text{con} \\quad\n",
        "E(S) = \\frac{1}{F T} \\sum_{f,t} S_{f,t}^2 $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eA9Rh0HOc1DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, weights=None, eps=1e-6, flat_win=9):\n",
        "\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.flat_win = flat_win\n",
        "\n",
        "        # Pesi bilanciati per enfatizzare alte frequenze e fedeltà strutturale\n",
        "        self.w = weights or {\n",
        "            \"charb\": 1.0,\n",
        "            \"hf\": 0.4,\n",
        "            \"dnh\": 0.25,\n",
        "            \"flat\":0.05,\n",
        "            \"bw\": 0.8,\n",
        "            \"energy\": 0.1\n",
        "        }\n",
        "\n",
        "\n",
        "        if weights is not None:\n",
        "            self.w.update(weights)\n",
        "\n",
        "    # Charbonnier loss\n",
        "    def charbonnier(self, x, y):\n",
        "        diff = x - y\n",
        "        return torch.mean(torch.sqrt(diff*diff + self.eps*self.eps))\n",
        "\n",
        "\n",
        "    # High-frequency\n",
        "    def hf_guided(self, pred, target):\n",
        "        B, C, F, T = pred.shape\n",
        "        # peso crescente lungo la frequenza\n",
        "        w_f = torch.linspace(0.8, 1.2, F, device=pred.device).view(1,1,F,1)  # [1,1,F,1]\n",
        "        # Normalizzazione del target per generare una maschera percettiva\n",
        "        t_norm = (target - target.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0]) \\\n",
        "                 / (target.max(dim=2, keepdim=True)[0].max(dim=3, keepdim=True)[0] -\n",
        "                    target.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0] + self.eps)\n",
        "        m = torch.clamp(t_norm, 0.0, 1.0)  # [B,1,F,T]\n",
        "        return torch.mean(w_f * m * torch.abs(pred - target))\n",
        "\n",
        "    # Do-no-harm\n",
        "    def do_no_harm(self, pred, target, degraded, tau=0.05):\n",
        "        # Calcola dove degraded è già simile al target\n",
        "        need = torch.abs(target - degraded)\n",
        "        need = need / (need.amax(dim=(2, 3), keepdim=True) + self.eps)\n",
        "        mask_no_need = 1.0 - need\n",
        "        # Penalizza modifiche inutili per evitare di peggiorare cose buone\n",
        "        return torch.mean(mask_no_need * torch.abs(pred - degraded))\n",
        "\n",
        "\n",
        "    # Spectral flatness penalty\n",
        "    def spectral_flatness(self, S):\n",
        "        # Evita log(0)\n",
        "        S = torch.clamp(S, min=self.eps)\n",
        "        # Media aritmetica e geometrica lungo il tempo\n",
        "        am = S.mean(dim=3)\n",
        "        gm = torch.exp((torch.log(S)).mean(dim=3))\n",
        "        sf = gm / (am + self.eps)\n",
        "        return sf\n",
        "\n",
        "    def tonal_penalty(self, pred, target):\n",
        "        # Differenza di flatness tra predizione e target\n",
        "        sf_p = self.spectral_flatness(pred)\n",
        "        sf_t = self.spectral_flatness(target)\n",
        "        return torch.mean(torch.abs(sf_p - sf_t))\n",
        "\n",
        "\n",
        "    # Band-weighted L1\n",
        "    def band_weighted_l1(self, pred, target):\n",
        "        B, C, F, T = pred.shape\n",
        "        # Curva sinusoidale\n",
        "        weights = 1.0 + 0.3 * torch.sin(torch.linspace(0, 3.14, F, device=pred.device))\n",
        "        weights = weights.view(1, 1, F, 1)\n",
        "        return torch.mean(weights * torch.abs(pred - target))\n",
        "\n",
        "\n",
        "    # Energy consistency\n",
        "    def energy_consistency(self, pred, target):\n",
        "        # Energia media per spettrogramma\n",
        "        e_pred = torch.mean(pred ** 2, dim=(2,3))\n",
        "        e_tgt  = torch.mean(target ** 2, dim=(2,3))\n",
        "        return torch.mean(torch.abs(e_pred - e_tgt))\n",
        "\n",
        "\n",
        "    # Forward\n",
        "    def forward(self, enhanced, target, degraded=None, return_components=False):\n",
        "        # Componenti principali della loss\n",
        "        charb = self.charbonnier(enhanced, target)\n",
        "        hf = self.hf_guided(enhanced, target)\n",
        "        bw = self.band_weighted_l1(enhanced, target)\n",
        "        energy = self.energy_consistency(enhanced, target)\n",
        "\n",
        "        # Somma pesata delle componenti principali\n",
        "        total = (\n",
        "            self.w[\"charb\"] * charb +\n",
        "            self.w[\"bw\"] * bw +\n",
        "            self.w[\"hf\"] * hf +\n",
        "            self.w[\"energy\"] * energy\n",
        "        )\n",
        "\n",
        "        # Componenti opzionali\n",
        "        dnh = None\n",
        "        if degraded is not None and self.w[\"dnh\"] > 0:\n",
        "            dnh = self.do_no_harm(enhanced, target, degraded)\n",
        "            total = total + self.w[\"dnh\"] * dnh\n",
        "\n",
        "        flat = None\n",
        "        if self.w[\"flat\"] > 0:\n",
        "            flat = self.tonal_penalty(enhanced, target)\n",
        "            total = total + self.w[\"flat\"] * flat\n",
        "\n",
        "        # Restituisce anche le singole componenti\n",
        "        if return_components:\n",
        "            out = {\"total\": total, \"charb\": charb, \"bw\": bw, \"hf\": hf, \"energy\": energy}\n",
        "            if dnh is not None:  out[\"dnh\"]  = dnh\n",
        "            if flat is not None: out[\"flat\"] = flat\n",
        "            return out\n",
        "        else:\n",
        "            return total\n"
      ],
      "metadata": {
        "id": "Nl9zAWwOYqwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "UzVB4XIhFnSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metriche"
      ],
      "metadata": {
        "id": "9oBIqBeYLOA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le metriche permettono di valutare quanto bene il modello sta facendo.\n",
        "Ho utilizzato quattro metriche, ognuna che guarda un aspetto diverso del segnale.\n",
        "\n",
        "---\n",
        "\n",
        "**Mean Squared Error**\n",
        "\n",
        "Misura la distanza quadratica media tra lo spettrogramma predetto $x$ e lo spettrogramma target $y$.\n",
        "\n",
        "Valori più bassi indicano una ricostruzione più fedele.\n",
        "\n",
        "$$MSE = \\frac{1}{N} \\sum_{f, t}\n",
        "(x_{f,t} - y_{f,t})^2 $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**L1, Mean Absolute Error**\n",
        "\n",
        "Indica quanto, in media, ogni punto delle spettrogramma differisce dal corrispondente valore del target. Permette di controllare la precisione media.\n",
        "\n",
        "Valori minori indicano una migliore ricostruzione.\n",
        "\n",
        "$$L1 = \\frac{1}{N} \\sum_{f, t}|x_{f,t} - y_{f,t}|$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Cosine Similarity**\n",
        "\n",
        "Misura l'orientamento tra i vettori. Vale:\n",
        "\n",
        "* $1$ se i due spettrogrammi hanno la stessa forma spettrale, anche se con differente scala;\n",
        "* $0$ se sono ortogonali;\n",
        "* $-1$ se sono opposti.\n",
        "\n",
        "Valori vicino a $1$ indicano una forte similarità strutturale.\n",
        "\n",
        "\n",
        "$$ CosSim = \\frac {\\mathbf{x} \\cdot \\mathbf{y}}\n",
        "{\\|\\mathbf{x}\\|_2 \\, \\|\\mathbf{y}\\|_2}\n",
        "= \\frac{\\sum_{i=1}^{N} x_i y_i}\n",
        "{\\sqrt{\\sum_{i=1}^{N} x_i^2} \\, \\sqrt{\\sum_{i=1}^{N} y_i^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQnAxwDTmk1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_spec(pred, target):\n",
        "    return torch.mean((pred - target) ** 2).item()\n",
        "\n",
        "\n",
        "def l1_spec(pred, target):\n",
        "    return torch.mean(torch.abs(pred - target)).item()\n",
        "\n",
        "\n",
        "def cosine_spec(pred, target):\n",
        "    pred_f = pred.flatten(start_dim=1).float()\n",
        "    target_f = target.flatten(start_dim=1).float()\n",
        "\n",
        "    # Centra i vettori (rimuove il bias positivo da [0,1])\n",
        "    pred_f = pred_f - pred_f.mean(dim=1, keepdim=True)\n",
        "    target_f = target_f - target_f.mean(dim=1, keepdim=True)\n",
        "\n",
        "    cos = F.cosine_similarity(pred_f, target_f, dim=1)\n",
        "    cos = torch.clamp(cos, -1.0, 1.0)\n",
        "    return torch.clamp(torch.mean(cos), -1.0, 1.0).item()\n",
        "\n"
      ],
      "metadata": {
        "id": "IypinqgDLRmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni Ausiliarie"
      ],
      "metadata": {
        "id": "enPSsv5LFpuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante l'elaborazione degli spettrogrammi lineari, è possibile che l'output prodotto dalla rete abbia dimensioni leggermente diverse rispetto al target. Queste discrepanze sono causate dalle convoluzioni, downsampling e upsampling, che possono modificare l'altezza o la larghezza del tensore.\n",
        "\n",
        "Per evitare errori quando calcoliamo la loss e le metriche, è necessario che la predizione del modello e il target abbiamo esattamente la stessa forma.\n",
        "\n",
        "La funzione `match_shape` uniforma le dimensioni dei due tensori. Confronta altezza e larghezza della predizione e del target. Se la dimensione della predizione è maggiore la taglia (crop), se è minore la riempie (pad) con zeri fino ad uguagliarla.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GfNvKhD8wYKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_shape(pred, target):\n",
        "    # Estrae le dimensioni di pred e target\n",
        "    _, _, h_p, w_p = pred.shape\n",
        "    _, _, h_t, w_t = target.shape\n",
        "\n",
        "    # Allinea l'altezza (frequenze)\n",
        "    if h_p > h_t:\n",
        "        pred = pred[:, :, :h_t, :]\n",
        "    elif h_p < h_t:\n",
        "        pred = F.pad(pred, (0, 0, 0, h_t - h_p))\n",
        "\n",
        "    # Allinea la larghezza (tempo)\n",
        "    if w_p > w_t:\n",
        "        pred = pred[:, :, :, :w_t]\n",
        "    elif w_p < w_t:\n",
        "        pred = F.pad(pred, (0, w_t - w_p, 0, 0))\n",
        "\n",
        "    return pred, target\n"
      ],
      "metadata": {
        "id": "3CAKJKZ5LWEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "XQ4AgrYGFs3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `train_spectral_model` implementa l’intero ciclo di addestramento del modello basato su spettrogrammi lineari.\n",
        "\n",
        "L’obiettivo è ottimizzare il modello nel ripristino dello spettro pulito a partire da versioni degradate, utilizzando un insieme di metriche quantitative (MSE, L1, COS, RMSE).\n",
        "\n",
        "La funzione è divisa in varie sezioni:\n",
        "\n",
        "1. **Inizializzazione**: mandiamo il modello sul dispositivo (`cuda` se disponibile altrimenti `cpu`). Creiamo un `GradScaler` per utilizzare mixed precision training, e inizializzaimo le variabili che conterranno le varie train loss, val loss e i valori delle metriche.\n",
        "\n",
        "2. **Loop per ogni epoca**: in ogni  epoca distinguiamo due fasi:\n",
        "    * **Training**: mettiamo il modello in modalità `train`. Per ogni batch calcoliamo la predizione (`enhanced`), la loss e aggiornaimo i pesi del modello.\n",
        "    * **Validation**: mettiamo il modello in modalità `eval`, calcoliamo la loss e le metriche. Confrontiamo i risultati tra `degraded` ed `enhanced` per valutare il miglioramento medio (`delta`).\n",
        "\n",
        "3. **Aggiormaneto dello scheduler**: se è definito un learning rate scheduler lo aggiorniamo in base alla validation loss.\n",
        "\n",
        "4. **Early Stopping e salvataggio Checkpoint**: se la `val_loss` migliora il modello viene salvato. Se il modello non migliora per `patience` epoche consecutive l'addestramenti si interrompe.\n",
        "\n",
        "5. **Visualizzazione plot**: se `show_example=True` vengono mostrati gli spettrogrammi clean, degraded ed enhanced. Se `show_metrics=True` vengono mostrati i valori delle metriche per valutare i progressi.\n"
      ],
      "metadata": {
        "id": "A4-1Jz0VwJQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spectral_model(\n",
        "    model,\n",
        "    train_dl,\n",
        "    val_dl,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler=None,\n",
        "    num_epochs=20,\n",
        "    patience=5,\n",
        "    device=\"cuda\",\n",
        "    save_path=\"/content/checkpoint_best.pt\",\n",
        "    show_example=False, # Per mostrare gli spettrogrammi\n",
        "    show_metrics=True, # Per mostrare le metriche\n",
        "    use_amp=True,\n",
        "    log_loss_components=False # Per mostrare i valori delle varie componenti della loss\n",
        "):\n",
        "    model = model.to(device)\n",
        "    scaler = amp.GradScaler('cuda', enabled=use_amp)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_losses, val_losses, val_metrics_list = [], [], []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n----- Epoch {epoch}/{num_epochs} -----\")\n",
        "\n",
        "\n",
        "        # TRAINING\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_bar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n",
        "\n",
        "        for spec_degraded, spec_clean in train_bar:\n",
        "            # Sposta su device\n",
        "            spec_degraded = spec_degraded.to(device, non_blocking=True)\n",
        "            spec_clean = spec_clean.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Forward\n",
        "            with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                # Predice enhanced\n",
        "                enhanced = model(spec_degraded)\n",
        "                enhanced = torch.clamp(enhanced, 0.0, 1.0)\n",
        "\n",
        "                # Allinea le dimensioni\n",
        "                enhanced, spec_clean = crop_to_match(enhanced, spec_clean)\n",
        "                spec_degraded, _ = crop_to_match(spec_degraded, spec_clean)\n",
        "\n",
        "                # Normalizzazione\n",
        "                ref = spec_clean.max()\n",
        "                ref = torch.clamp(ref, min=1e-8)  # sicurezza\n",
        "                spec_clean_n = spec_clean / ref\n",
        "                spec_degraded_n = spec_degraded / ref\n",
        "                enhanced_n = enhanced / ref\n",
        "\n",
        "                loss = loss_fn(enhanced_n, spec_clean_n, spec_degraded_n)\n",
        "\n",
        "            # Controlla stabilità numerica\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\" NaN/Inf rilevato — training interrotto.\")\n",
        "                return model, train_losses, val_losses, val_metrics_list\n",
        "\n",
        "            # Backpropagation\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.detach().item()\n",
        "            train_bar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= max(1, len(train_dl))\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        metrics_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0}\n",
        "        n_batches = 0\n",
        "        do_no_harm_ratios = []  # Debug\n",
        "\n",
        "        val_bar = tqdm(val_dl, desc=f\"Val {epoch}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            delta_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0}\n",
        "\n",
        "            for spec_degraded, spec_clean in val_bar:\n",
        "                spec_degraded = spec_degraded.to(device, non_blocking=True)\n",
        "                spec_clean = spec_clean.to(device, non_blocking=True)\n",
        "\n",
        "                with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                    enhanced = model(spec_degraded)\n",
        "                    enhanced = torch.clamp(enhanced, 0.0, 1.0)\n",
        "\n",
        "                    enhanced, spec_clean = crop_to_match(enhanced, spec_clean)\n",
        "                    spec_degraded, _ = crop_to_match(spec_degraded, spec_clean)\n",
        "\n",
        "                    # Normalizzazione\n",
        "                    ref = spec_clean.max()\n",
        "                    ref = torch.clamp(ref, min=1e-8)\n",
        "                    spec_clean_n = spec_clean / ref\n",
        "                    spec_degraded_n = spec_degraded / ref\n",
        "                    enhanced_n = enhanced / ref\n",
        "\n",
        "                    if log_loss_components:\n",
        "                        out = loss_fn(enhanced_n, spec_clean_n, degraded=spec_degraded_n, return_components=True)\n",
        "                        loss = out[\"total\"]\n",
        "                        comp = {k: v.item() if torch.is_tensor(v) else v for k, v in out.items()}\n",
        "                    else:\n",
        "                        loss = loss_fn(enhanced_n, spec_clean_n, spec_degraded_n)\n",
        "                        comp = None\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calcola le metriche\n",
        "                restored_metrics = {\n",
        "                    \"MSE\": mse_spec(enhanced_n.cpu(), spec_clean_n.cpu()),\n",
        "                    \"L1\": l1_spec(enhanced_n.cpu(), spec_clean_n.cpu()),\n",
        "                    \"COS\": cosine_spec(enhanced_n.cpu(), spec_clean_n.cpu())\n",
        "                }\n",
        "\n",
        "                baseline_metrics = {\n",
        "                    \"MSE\": mse_spec(spec_degraded_n.cpu(), spec_clean_n.cpu()),\n",
        "                    \"L1\": l1_spec(spec_degraded_n.cpu(), spec_clean_n.cpu()),\n",
        "                    \"COS\": cosine_spec(spec_degraded_n.cpu(), spec_clean_n.cpu())\n",
        "                }\n",
        "\n",
        "                # Calcola il delta Δ\n",
        "                delta_metrics = {k: baseline_metrics[k] - restored_metrics[k] for k in restored_metrics}\n",
        "\n",
        "\n",
        "                for k in metrics_sum.keys():\n",
        "                    metrics_sum[k] += restored_metrics[k]\n",
        "                    delta_sum[k] += delta_metrics[k]\n",
        "\n",
        "\n",
        "                n_batches += 1\n",
        "                val_bar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        val_loss /= n_batches\n",
        "        metrics_avg = {k: v / n_batches for k, v in metrics_sum.items()}\n",
        "        avg_delta = {k: delta_sum[k] / n_batches for k in delta_sum.keys()}\n",
        "\n",
        "        print(f\"\\nΔ delta medio per epoca — \"\n",
        "              f\"MSE:{avg_delta['MSE']:+.5f} | \"\n",
        "              f\"L1:{avg_delta['L1']:+.5f} | \"\n",
        "              f\"COS:{avg_delta['COS']:+.5f}\" )\n",
        "\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_metrics_list.append(metrics_avg)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        # EARLY STOPPING + CHECKPOINT\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"metrics\": metrics_avg,\n",
        "            }, save_path)\n",
        "            print(f\"Nuovo best model salvato (val_loss={val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Nessun miglioramento ({patience_counter}/{patience})\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping attivato alla epoca {epoch}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        # VISUALIZZAZIONE ESEMPIO\n",
        "        if show_example:\n",
        "            spec_degraded, spec_clean = next(iter(val_dl))\n",
        "            spec_degraded, spec_clean = spec_degraded.to(device), spec_clean.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                enhanced = model(spec_degraded)\n",
        "                enhanced = torch.clamp(enhanced, 0.0, 1.0)  # clamping anche qui\n",
        "\n",
        "\n",
        "            # Normalizzazione\n",
        "            ref = spec_clean.max()\n",
        "            ref = torch.clamp(ref, min=1e-8)\n",
        "            spec_clean_n = spec_clean / ref\n",
        "            spec_degraded_n = spec_degraded / ref\n",
        "            enhanced_n = enhanced / ref\n",
        "\n",
        "            idx = 0\n",
        "            plt.figure(figsize=(14, 8))\n",
        "\n",
        "            plt.subplot(3, 1, 1)\n",
        "            im1 = plt.imshow(spec_clean_n[idx, 0].cpu().numpy() ** 0.5,\n",
        "                            origin='lower', aspect='auto', cmap='viridis')\n",
        "            plt.title(\"Clean (Target)\")\n",
        "            plt.colorbar(im1, fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.subplot(3, 1, 2)\n",
        "            im2 = plt.imshow(spec_degraded_n[idx, 0].cpu().numpy() ** 0.5,\n",
        "                            origin='lower', aspect='auto', cmap='viridis')\n",
        "            plt.title(\"Degraded (Input)\")\n",
        "            plt.colorbar(im2, fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.subplot(3, 1, 3)\n",
        "            im3 = plt.imshow(enhanced_n[idx, 0].cpu().numpy() ** 0.5,\n",
        "                            origin='lower', aspect='auto', cmap='viridis',\n",
        "                            vmin=0, vmax=1)  # Forza range visivo coerente\n",
        "            plt.title(\"Enhanced (Output)\")\n",
        "            plt.colorbar(im3, fraction=0.046, pad=0.04)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        # LOGGING\n",
        "        if show_metrics:\n",
        "            print(f\"\\nValidation Summary (Epoch {epoch}) \")\n",
        "            print(f\"Val Loss: {val_loss:.4f}\")\n",
        "            print(f\"MSE:{metrics_avg['MSE']:.5f} | L1:{metrics_avg['L1']:.5f} | COS:{metrics_avg['COS']:.5f} \")\n",
        "\n",
        "            if log_loss_components and comp is not None:\n",
        "                print(f\"Loss components --> Total:{comp['total']:.4f} | \"\n",
        "                      f\"charb:{comp['charb']:.4f} | hf:{comp['hf']:.4f} | \"\n",
        "                      f\"bw:{comp['bw']:.4f} | energy:{comp['energy']:.4f} |\"\n",
        "                      f\"dnh:{comp['dnh']:.4f} | flat:{comp['flat']:.4f} \"\n",
        "                )\n",
        "            print(\"-------------------------------------------\")\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nTraining completato — Best epoch: {best_epoch} | Best val_loss={best_val_loss:.4f}\")\n",
        "    return model, train_losses, val_losses, val_metrics_list\n"
      ],
      "metadata": {
        "id": "DUe_YFtILm9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ATTENZIONE!!!** Se la variabile `skip_training` all'interno del dizionario `CONFIG` è stata impostata a `False` i pesi scaricati dalla repo github verranno sovrascritti."
      ],
      "metadata": {
        "id": "3MEpW5hSXmJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Gruppo A"
      ],
      "metadata": {
        "id": "XAd4JyBYFvgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializziamo il modello\n",
        "model_A = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model_A.parameters(),\n",
        "    lr=1e-4,\n",
        "    betas=(0.9, 0.98),\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=5e-6\n",
        ")\n",
        "\n",
        "path_gruppoA = \"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\"\n",
        "\n",
        "\n",
        "if CONFIG[\"skip_training\"] == True and os.path.exists(path_gruppoA):\n",
        "    checkpoint = torch.load(path_gruppoA, map_location=device)\n",
        "    model_A.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"\\nTraining saltato, caricamento checkpoint da: {path_gruppoA}\")\n",
        "\n",
        "elif CONFIG[\"skip_training\"] == True:\n",
        "    print(f\"\\nRichiesto skip training ma il checkpoint {path_gruppoA} non esiste. Avvio del training.\")\n",
        "    model_A, train_losses_A, val_losses_A, val_metrics_A = train_spectral_model(\n",
        "        model=model_A,\n",
        "        train_dl=train_loader_A,\n",
        "        val_dl=val_loader_A,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )\n",
        "\n",
        "else:\n",
        "    model_A, train_losses_A, val_losses_A, val_metrics_A = train_spectral_model(\n",
        "        model=model_A,\n",
        "        train_dl=train_loader_A,\n",
        "        val_dl=val_loader_A,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )"
      ],
      "metadata": {
        "id": "0_0b5PyiLrud",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Gruppo B"
      ],
      "metadata": {
        "id": "nfnIlrMYF09a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializziamo il modello\n",
        "model_B = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model_B.parameters(),\n",
        "    lr=1e-4,\n",
        "    betas=(0.9, 0.98),\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=5e-6\n",
        ")\n",
        "\n",
        "path_gruppoB = \"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\"\n",
        "\n",
        "\n",
        "if CONFIG[\"skip_training\"] == True and os.path.exists(path_gruppoB):\n",
        "    checkpoint = torch.load(path_gruppoB, map_location=device)\n",
        "    model_B.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"\\nTraining saltato, caricamento checkpoint da: {path_gruppoB}\")\n",
        "\n",
        "elif CONFIG[\"skip_training\"] == True:\n",
        "    print(f\"\\nRichiesto skip training ma il checkpoint {path_gruppoB} non esiste. Avvio del training.\")\n",
        "\n",
        "    model_B, train_losses_B, val_losses_B, val_metrics_B = train_spectral_model(\n",
        "        model=model_B,\n",
        "        train_dl=train_loader_B,\n",
        "        val_dl=val_loader_B,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )\n",
        "\n",
        "\n",
        "else:\n",
        "    model_B, train_losses_B, val_losses_B, val_metrics_B = train_spectral_model(\n",
        "      model=model_B,\n",
        "      train_dl=train_loader_B,\n",
        "      val_dl=val_loader_B,\n",
        "      loss_fn=loss_fn,\n",
        "      optimizer=optimizer,\n",
        "      scheduler=scheduler,\n",
        "      num_epochs=25,\n",
        "      patience=5,\n",
        "      device=device,\n",
        "      save_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\",\n",
        "      show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "      show_metrics=True, # Mostra le metriche\n",
        "      use_amp=True, # Mixed precision\n",
        "      log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "  )"
      ],
      "metadata": {
        "id": "VxGE1YNXLx__",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "fJm69g_PF5Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante la fase di Test e valutazione finale dobbiamo misurare quanto bene il modello ha ricostruito gli spettrogrammi.\n",
        "\n"
      ],
      "metadata": {
        "id": "B_p7lzGa8_5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni Ausiliarie"
      ],
      "metadata": {
        "id": "ycdePGvNMJL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `spec_to_audio` ricostruisce un segnale audio a partire da uno spettrogramma normalizzato nel range $[0, 1]$.\n",
        "\n",
        "1. Converte i valori da [0,1] --> dB --> ampiezza lineare.\n",
        "\n",
        "2. Applica la trasformata inversa di Fourier tramite l’algoritmo di Griffin-Lim, che stima iterativamente la fase mancante dello spettro.\n",
        "\n",
        "3. Normalizza il segnale risultante per evitare clipping.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "83QFQgUVz9Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione di ricostruzione audio\n",
        "def spec_to_audio(spec_01, griffinlim_iters):\n",
        "\n",
        "    # Da [0,1] --> dB --> ampiezza lineare\n",
        "    S_db = spec_01 * CONFIG[\"top_db\"] - CONFIG[\"top_db\"]\n",
        "    S_amp = librosa.db_to_amplitude(S_db)\n",
        "\n",
        "    # Finestra identica a quella usata nello STFT originale\n",
        "    win = get_window(CONFIG[\"window\"], CONFIG[\"win_length\"], fftbins=True)\n",
        "\n",
        "    # Ricostruzione con Griffin-Lim\n",
        "    audio = librosa.griffinlim(\n",
        "        S_amp,\n",
        "        n_iter=griffinlim_iters,\n",
        "        hop_length=CONFIG[\"hop_length\"],\n",
        "        win_length=CONFIG[\"win_length\"],\n",
        "        window=win,\n",
        "        center=CONFIG[\"center\"]\n",
        "    )\n",
        "\n",
        "    # Normalizzazione\n",
        "    audio = audio / (np.max(np.abs(audio)) + CONFIG[\"epsilon\"])\n",
        "    return audio"
      ],
      "metadata": {
        "id": "KqWu40GmMPzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop"
      ],
      "metadata": {
        "id": "FzF2gThmF7YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funzione `test_spectral_model` eseguiamo la valutazione finale del modello sul set di test.\n",
        "L’obiettivo è misurare le prestazioni del modello addestrato confrontando gli spettrogrammi enhanced con quelli clean, e verificare quanto riesca a migliorare rispetto ai degraded.\n",
        "\n",
        "All’inizio viene caricato il checkpoint migliore salvato durante il training.\n",
        "Il modello viene impostato in modalità `eval` e viene inizializzato un `GradScaler`.\n",
        "Vengono poi creati i dizionari per accumulare i valori medi di loss e metriche (MSE, L1, COS).\n",
        "\n",
        "Per ogni batch:\n",
        "\n",
        "1. il modello genera lo spettrogramma `enhanced` a partire da `degraded`;\n",
        "\n",
        "2. i tensori vengono allineati e normalizzati;\n",
        "\n",
        "3. calcoliamo la loss complessiva;\n",
        "\n",
        "4. calcoliamo le metriche;\n",
        "\n",
        "5. confrontiamo le metriche di degraded ed enhanced calcolando il delta (Δ) per ogni metrica.\n",
        "\n",
        "Inoltre, per un batch selezionato, vengono visualizzati e ascoltati gli spettrogrammi clean, degraded e enhanced, ricostruiti tramite algoritmo di Griffin Lim.\n"
      ],
      "metadata": {
        "id": "rs-E3zWR9p-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_spectral_model(\n",
        "    model,\n",
        "    test_dl,\n",
        "    loss_fn,\n",
        "    checkpoint_path=\"/content/checkpoint_best.pt\",\n",
        "    device=\"cuda\",\n",
        "    griffinlim_iters=32,\n",
        "    save_audio=False,\n",
        "    save_dir=\"/content/audio_tests\",\n",
        "    listen_to_first=True,\n",
        "    log_loss_components=False,\n",
        "    use_amp=True\n",
        "):\n",
        "\n",
        "\n",
        "    # Caricamento checkpoint\n",
        "    print(f\"\\nCaricamento checkpoint da: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Checkpoint caricato (epoch {checkpoint['epoch']}, val_loss={checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "\n",
        "    # Inizializza metriche\n",
        "    total_loss = 0.0\n",
        "    metrics_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0}\n",
        "    baseline_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0}\n",
        "    delta_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0}\n",
        "    n_batches = 0\n",
        "\n",
        "\n",
        "    # Loop di test\n",
        "    print(\"\\nInizio test su set di test...\")\n",
        "    test_bar = tqdm(test_dl, desc=\"Testing\", leave=False)\n",
        "\n",
        "    scaler = amp.GradScaler('cuda', enabled=use_amp)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (spec_degraded, spec_clean) in enumerate(test_bar):\n",
        "            spec_degraded = spec_degraded.to(device, non_blocking=True)\n",
        "            spec_clean = spec_clean.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward\n",
        "            with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                enhanced = model(spec_degraded)\n",
        "                enhanced = torch.clamp(enhanced, 0.0, 1.0)\n",
        "                enhanced, spec_clean = crop_to_match(enhanced, spec_clean)\n",
        "                spec_degraded, spec_clean = crop_to_match(spec_degraded, spec_clean)\n",
        "\n",
        "                # Normalizzazione\n",
        "                ref = spec_clean.max()\n",
        "                ref = torch.clamp(ref, min=1e-8)\n",
        "                spec_clean_n = spec_clean / ref\n",
        "                spec_degraded_n = spec_degraded / ref\n",
        "                enhanced_n = enhanced / ref\n",
        "\n",
        "                # Loss\n",
        "                if log_loss_components:\n",
        "                    out = loss_fn(enhanced_n, spec_clean_n, degraded=spec_degraded_n, return_components=True)\n",
        "                    loss = out[\"total\"]\n",
        "                    comp = {k: v.item() for k, v in out.items()}\n",
        "                else:\n",
        "                    loss = loss_fn(enhanced_n, spec_clean_n, spec_degraded_n)\n",
        "                    comp = None\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Metriche restored (enhanced vs clean)\n",
        "            restored_metrics = {\n",
        "                \"MSE\": mse_spec(enhanced.cpu(), spec_clean.cpu()),\n",
        "                \"L1\": l1_spec(enhanced.cpu(), spec_clean.cpu()),\n",
        "                \"COS\": cosine_spec(enhanced.cpu(), spec_clean.cpu())\n",
        "            }\n",
        "\n",
        "            # Baseline (degraded vs clean)\n",
        "            baseline_metrics = {\n",
        "                \"MSE\": mse_spec(spec_degraded.cpu(), spec_clean.cpu()),\n",
        "                \"L1\": l1_spec(spec_degraded.cpu(), spec_clean.cpu()),\n",
        "                \"COS\": cosine_spec(spec_degraded.cpu(), spec_clean.cpu())\n",
        "            }\n",
        "\n",
        "            # Delta Δ baseline - restored\n",
        "            delta_metrics = {k: baseline_metrics[k] - restored_metrics[k] for k in restored_metrics}\n",
        "\n",
        "            for k in metrics_sum.keys():\n",
        "                metrics_sum[k] += restored_metrics[k]\n",
        "                baseline_sum[k] += baseline_metrics[k]\n",
        "                delta_sum[k] += delta_metrics[k]\n",
        "\n",
        "            n_batches += 1\n",
        "            test_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "            # Visualizzazione e ascolto\n",
        "            if listen_to_first and batch_idx == 103:\n",
        "                degraded_np = spec_degraded.squeeze().cpu().numpy()\n",
        "                clean_np = spec_clean.squeeze().cpu().numpy()\n",
        "                enhanced_np = enhanced.squeeze().cpu().numpy()\n",
        "\n",
        "                enhanced_np = np.clip(enhanced_np / (enhanced_np.max() + 1e-8), 0.0, 1.0)\n",
        "\n",
        "\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                plt.subplot(3, 1, 1)\n",
        "                plt.imshow(clean_np ** 0.5, origin='lower', aspect='auto', cmap='viridis')\n",
        "                plt.title(\"Clean (Target)\")\n",
        "                plt.colorbar()\n",
        "\n",
        "                plt.subplot(3, 1, 2)\n",
        "                plt.imshow(degraded_np ** 0.5, origin='lower', aspect='auto', cmap='viridis')\n",
        "                plt.title(\"Degraded (Input)\")\n",
        "                plt.colorbar()\n",
        "\n",
        "                plt.subplot(3, 1, 3)\n",
        "                plt.imshow(enhanced_np ** 0.5, origin='lower', aspect='auto', cmap='viridis')\n",
        "                plt.title(\"Enhanced (Output)\")\n",
        "                plt.colorbar()\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Griffin-Lim\n",
        "                y_clean = spec_to_audio(clean_np, griffinlim_iters)\n",
        "                y_degraded = spec_to_audio(degraded_np, griffinlim_iters)\n",
        "                y_enhanced = spec_to_audio(enhanced_np, griffinlim_iters)\n",
        "\n",
        "                print(\"\\nClean (Target)\")\n",
        "                display(Audio(y_clean, rate=CONFIG[\"sample_rate\"]))\n",
        "                print(\"Degraded (Input)\")\n",
        "                display(Audio(y_degraded, rate=CONFIG[\"sample_rate\"]))\n",
        "                print(\"Enhanced (Output)\")\n",
        "                display(Audio(y_enhanced, rate=CONFIG[\"sample_rate\"]))\n",
        "\n",
        "                if save_audio:\n",
        "                    import os, soundfile as sf\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    sf.write(f\"{save_dir}/clean.wav\", y_clean, CONFIG[\"sample_rate\"])\n",
        "                    sf.write(f\"{save_dir}/degraded.wav\", y_degraded, CONFIG[\"sample_rate\"])\n",
        "                    sf.write(f\"{save_dir}/enhanced.wav\", y_enhanced, CONFIG[\"sample_rate\"])\n",
        "                    print(f\"Audio salvato in: {save_dir}\")\n",
        "\n",
        "\n",
        "    # Calcolo medie e report\n",
        "    avg_loss = total_loss / n_batches\n",
        "    restored = {k: metrics_sum[k] / n_batches for k in metrics_sum}\n",
        "    baseline = {k: baseline_sum[k] / n_batches for k in baseline_sum}\n",
        "    delta = {k: delta_sum[k] / n_batches for k in delta_sum}\n",
        "\n",
        "    print(\"\\nRISULTATI MEDI SU TEST SET:\")\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "    for k in restored.keys():\n",
        "        print(f\"{k}: base={baseline[k]:.5f} | restored={restored[k]:.5f} | Δ={delta[k]:+.5f}\")\n",
        "\n",
        "    if log_loss_components and comp is not None:\n",
        "        print(\"\\nUltime componenti di loss viste:\")\n",
        "        print(f\"Total: {comp['total']:.4f} | Charb: {comp['charb']:.4f} | HF: {comp['hf']:.4f}\")\n",
        "\n",
        "    return baseline, restored, delta"
      ],
      "metadata": {
        "id": "i9E8vDDVL5sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop su Modello A"
      ],
      "metadata": {
        "id": "4D8WqpeaGD_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo A"
      ],
      "metadata": {
        "id": "s0vD2vyxGJsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_A = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_AA, restored_AA, delta_AA = test_spectral_model(\n",
        "    model=model_A,\n",
        "    test_dl=test_loader_A,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\",\n",
        "    device=\"cuda\",\n",
        "    griffinlim_iters=64,\n",
        "    save_audio=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2WquW7zWMxDE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo B"
      ],
      "metadata": {
        "id": "1z4FfUTKGN7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_A = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_AB, restored_AB, delta_AB = test_spectral_model(\n",
        "    model=model_A,\n",
        "    test_dl=test_loader_B,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\",\n",
        "    device=\"cuda\",\n",
        "    griffinlim_iters=64,\n",
        "    save_audio=False\n",
        ")"
      ],
      "metadata": {
        "id": "RowezC0_M0oG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop su Modello B"
      ],
      "metadata": {
        "id": "69Dd-_AEGQsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo A"
      ],
      "metadata": {
        "id": "TG3L3fiOGQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_B = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_BA, restored_BA, delta_BA = test_spectral_model(\n",
        "    model=model_B,\n",
        "    test_dl=test_loader_A,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\",\n",
        "    device=\"cuda\",\n",
        "    griffinlim_iters=64,\n",
        "    save_audio=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "xhSVeaq1NYwU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo B"
      ],
      "metadata": {
        "id": "S2BmxlpMGQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_B = SpectralEnhancementNet().to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_BB, restored_BB, delta_BB = test_spectral_model(\n",
        "    model=model_B,\n",
        "    test_dl=test_loader_B,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\",\n",
        "    device=\"cuda\",\n",
        "    griffinlim_iters=64,\n",
        "    save_audio=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "eiFyw8QwNJ38",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prova su output di MusicGen"
      ],
      "metadata": {
        "id": "YsjYafwhGpCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qui ho deciso di testare il modello addestrato sul gruppo A e il modello addestrato sul gruppo B su un output di MusciGen."
      ],
      "metadata": {
        "id": "P0Sk8bsMAAM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate torchaudio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NVNCmccc9eM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "\n",
        "print(\"Generazione audio con MusicGen\")\n",
        "# Carica modello e processor di Meta MusicGen\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "musicgen = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").to(device)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4cLLoXtg95qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt = \"jazz\"\n",
        "\n",
        "# Genera audio\n",
        "inputs = processor(\n",
        "    text=[prompt],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "audio_values = musicgen.generate(**inputs, max_new_tokens=256)\n",
        "\n",
        "# Salva file audio\n",
        "sf.write(\"/content/musicgen_output.wav\", audio_values[0, 0].cpu().numpy(), 32000)\n",
        "print(\"File generato: /content/musicgen_output.wav\")\n",
        "\n",
        "display(Audio(\"/content/musicgen_output.wav\", rate=32000))"
      ],
      "metadata": {
        "id": "n_nJh9Q3Eb3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_musicgen_output(\n",
        "    model,\n",
        "    path_audio,\n",
        "    save_path=\"/content/enhanced_musicgen.wav\",\n",
        "    griffinlim_iters=64,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Caricamento audio\n",
        "    audio, _ = librosa.load(path_audio, sr=CONFIG[\"sample_rate\"], mono=True)\n",
        "    print(f\"Audio caricato: {path_audio} — Durata: {len(audio) / CONFIG['sample_rate']:.2f}s\")\n",
        "\n",
        "\n",
        "    # STFT e normalizzazione\n",
        "    win = scipy.signal.get_window(CONFIG[\"window\"], CONFIG[\"win_length\"], fftbins=True)\n",
        "    S = librosa.stft(\n",
        "        audio,\n",
        "        n_fft=CONFIG[\"n_fft\"],\n",
        "        hop_length=CONFIG[\"hop_length\"],\n",
        "        win_length=CONFIG[\"win_length\"],\n",
        "        window=win,\n",
        "        center=CONFIG[\"center\"]\n",
        "    )\n",
        "    Mag = np.abs(S)\n",
        "\n",
        "    # Conversione in dB e normalizzazione [0,1]\n",
        "    S_db = librosa.amplitude_to_db(np.maximum(Mag, CONFIG[\"epsilon\"]), ref=np.max, top_db=CONFIG[\"top_db\"])\n",
        "    S_norm = (S_db + CONFIG[\"top_db\"]) / CONFIG[\"top_db\"]\n",
        "    S_norm = np.clip(S_norm, 0.0, 1.0)\n",
        "\n",
        "\n",
        "    # Conversione in tensore e normalizzazione\n",
        "    spec_input = torch.from_numpy(S_norm).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "\n",
        "    # Inference con clamp\n",
        "    with torch.no_grad(), amp.autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
        "        # Riferimento\n",
        "        ref = spec_input.max()\n",
        "        ref = torch.clamp(ref, min=1e-8)\n",
        "        spec_input_n = spec_input / ref\n",
        "\n",
        "        enhanced = model(spec_input_n)\n",
        "        enhanced = torch.clamp(enhanced, 0.0, 1.0)\n",
        "\n",
        "\n",
        "    print(f\"enhanced range: [{enhanced.min().item():.4f}, {enhanced.max().item():.4f}]\")\n",
        "    enhanced_np = enhanced.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "    # Ricostruzione audio\n",
        "    enhanced_audio = spec_to_audio(enhanced_np, griffinlim_iters)\n",
        "\n",
        "    # Normalizza l’output audio\n",
        "    enhanced_audio = enhanced_audio / (np.max(np.abs(enhanced_audio)) + CONFIG[\"epsilon\"])\n",
        "\n",
        "\n",
        "    # Visualizzazione\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.imshow(S_norm ** 0.5, origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=1)\n",
        "    plt.title(\"Input (MusicGen Output)\")\n",
        "    plt.colorbar()\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.imshow(enhanced_np ** 0.5, origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=1)\n",
        "    plt.title(\"Enhanced by Model\")\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Ascolto e confronto\n",
        "    print(\"\\nAudio Originale (MusicGen):\")\n",
        "    display(Audio(audio, rate=CONFIG[\"sample_rate\"]))\n",
        "    print(\"Audio Migliorato (Enhanced):\")\n",
        "    display(Audio(enhanced_audio, rate=CONFIG[\"sample_rate\"]))\n",
        "\n",
        "\n",
        "    # Salvataggio\n",
        "    sf.write(save_path, enhanced_audio, CONFIG[\"sample_rate\"])\n",
        "    print(f\"\\nOutput del modello salvato in: {save_path}\")\n"
      ],
      "metadata": {
        "id": "FOH5htJLPLnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello addestrato sul Gruppo A"
      ],
      "metadata": {
        "id": "OIrJ8V2IPYCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_A = SpectralEnhancementNet().to(device)\n",
        "ckpt_A = torch.load(\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoA.pt\", map_location=device)\n",
        "model_A.load_state_dict(ckpt_A[\"model_state_dict\"])\n",
        "print(f\"Modello addestrato sul Gruppo A caricato (epoch {ckpt_A['epoch']} | val_loss={ckpt_A['val_loss']:.4f})\")\n"
      ],
      "metadata": {
        "id": "nwA8P_eK_vCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhancemnet con il modello\n",
        "enhance_musicgen_output(model_A, \"/content/musicgen_output.wav\", save_path=\"/content/enhanced_A.wav\")"
      ],
      "metadata": {
        "id": "46AE1I5VBPG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello addestrato sul Gruppo B"
      ],
      "metadata": {
        "id": "8nCoAZYsPcLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_B = SpectralEnhancementNet().to(device)\n",
        "ckpt_B = torch.load(\"/content/projectID2ML/modello_lineare/checkpoint_best_speclin_gruppoB.pt\", map_location=device)\n",
        "model_B.load_state_dict(ckpt_B[\"model_state_dict\"])\n",
        "print(f\"Modello addestrato sul Gruppo B caricato (epoch {ckpt_B['epoch']} | val_loss={ckpt_B['val_loss']:.4f})\")\n"
      ],
      "metadata": {
        "id": "VIaxrL9BAeFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhancemnet con il modello\n",
        "enhance_musicgen_output(model_B, \"/content/musicgen_output.wav\", save_path=\"/content/enhanced_B.wav\")"
      ],
      "metadata": {
        "id": "mxjUuaGMAuTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi dei Risultati"
      ],
      "metadata": {
        "id": "wDTza6H8GZXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ho addestrato il modello `Spectral Enhancement` su due domini diversi, ciascuno dedicato a una diversa famiglia di degradazioni spettrali.\n",
        "Il primo modello è stato addestrato sul **gruppo A** (quantize, tonal stripes, noise), caratterizzato da degradazioni additive o periodiche, mentre il secondo sul **gruppo B** (clipping, reverb, lowpass, distort), che comprende distorsioni non lineari o di tipo convolutivo."
      ],
      "metadata": {
        "id": "ZK852sJ_CnJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modello Addestrato sul gruppo A**\n",
        "\n",
        "Il modello addestrato sul gruppo A mostra una mostra una rapida discesa iniziale della validation loss.\n",
        "La rete impara rapidamente le caratteristiche principali delle degradazioni additive, migliorando progressivamente le componenti della loss.\n",
        "\n",
        "Già dalle prime epoche si osserva una buona coerenza spettrale, con gli spettrogrammi enhanced che recuperano le armoniche principali e riducono il rumore di fondo.\n",
        "Tuttavia, la rete mostra una leggera tendenza alla sovra-levigazione, evidente nelle ultime epoche, dove alcune strutture spettrali fini vengono attenuate, portando a una lieve perdita di dettaglio. Questo effetto è coerente con l’azione regolarizzante della loss combinata e con l’uso di normalizzazione e mixed precision.\n",
        "\n",
        "È importante notare che durante l’addestramento la validation loss risulta quasi sempre inferiore alla training loss. Questo comportamento, sebbene atipico, può avere diverse spiegazioni, come la presenza di dropout o data augmentation nel training può rendere il modello più \"rumoroso\" in fase di apprendimento rispetto alla validazione.\n",
        "\n",
        "\n",
        "Nel test sullo stesso dominio (*gruppo A*), il modello mostra una chiara capacità di generalizzazione, le metriche migliorano. Visivamente, gli spettrogrammi enhanced risultano più puliti e regolari, con una riduzione del rumore.\n",
        "\n",
        "Nel complesso, il modello addestrato sul gruppo A si comporta in modo stabile ed efficace, riduce il rumore e ricostruisce le componenti tonali principali, sacrificando leggermente i dettagli fini per ottenere un segnale più regolare.\n",
        "\n",
        "\n",
        "Il test out-of-domain (cioè testando il modello A sul *gruppo B*) evidenzia invece la scarsa capacità di trasferimento del modello addestrato su degradazioni additive.\n",
        "Le metriche mostrano un peggioramento generale.\n",
        "\n",
        "Gli spettrogrammi rivelano che il modello non riesce a compensare distorsioni di tipo non lineare come il clipping o convolutive come il reverb.\n",
        "L’output appare quindi meno coerente e, in media, non migliora rispetto al segnale degradato.\n",
        "\n",
        "Questo comportamento conferma che il modello, pur efficace in-domain, non generalizza bene verso degradazioni di natura diversa, poiché ha appreso rappresentazioni specifiche del rumore additive e stazionarie.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EPlaRfctDeZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modello Addestrato sul gruppo B**\n",
        "\n",
        "Il modello addestrato sul gruppo B mostra un andamento di apprendimento più graduale e irregolare rispetto a quello addestrato sul gruppo A.\n",
        "\n",
        "Anche in questo caso, la validation loss risulta sempre inferiore alla training loss. Questo comportamento può essere attribuito all’effetto regolarizzante di dropout e normalizzazione, o al fatto che il validation set presenti una variabilità minore o una minore intensità delle degradazioni rispetto al training set.\n",
        "\n",
        "\n",
        "Nel test sullo stesso dominio (*gruppo B*), il modello mostra un comportamento coerente con quello osservato in validazione, le metriche mostrano lievi miglioramenti.\n",
        "\n",
        "Il test out-of-domain (*cioè testando il modello B sul gruppo A*) mostra un calo delle prestazioni, con metriche peggiorate rispetto al degradato.\n",
        "Il modello, avendo appreso pattern specifici delle degradazioni del gruppo B, non generalizza efficacemente su fenomeni di natura additiva come quantizzazione o rumore gaussiano.\n",
        "\n",
        "Nel complesso, anche in questo caso, il modello si dimostra efficace sulle degradazioni del propio dominio, ma limitato nella capacità di generalizzare fuori dominio."
      ],
      "metadata": {
        "id": "nhhGzk4PLmc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possibili Miglioramenti**\n",
        "\n",
        "Per migliorare la robustezza cross-domain, un’estensione futura potrebbe includere strategie di continual learning (es. weight regularization o replay di esempi di entrambi i gruppi), per consentire al modello di adattarsi progressivamente a nuove degradazioni senza perdere le conoscenze apprese.\n"
      ],
      "metadata": {
        "id": "3O44ASzyLpKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test su Output di MusicGen**\n",
        "\n",
        "Ho deciso di testare il mio modello su un output di MusicGen, anche se questi output sono già molto buoni. Ovviamente qui non abbiamo una traccia clean su cui poterci basare, quindi possiamo fare un'analisi qualitativa, visualizzando gli spettrogrammi e ascoltando i due audio.\n",
        "\n",
        "Il modello addestrato sul **gruppo A** mostra un comportamento conservativo, riduce artefatti a bassa intensità, come il rumore costante, senza compromettere la struttura armonica principale. Questo approccio è efficace quando gli artefatti di MusicGen sono di natura additiva.\n",
        "\n",
        "Il modello addestrato sul **gruppo B** tende a intervenire in modo più \"deciso\". Riduce componenti spurie o distorsioni percepite, come clipping, ma può anche attenuare eccessivamente le alte frequenze, generando un suono più morbido ma meno brillante.\n",
        "Questo comportamento è coerente con il tipo di degradazioni su cui è stato addestrato, il modello interpreta parte della brillantezza sintetica di MusicGen come distorsione e la sopprime.\n",
        "\n",
        "\n",
        "Entrambi i modelli mantengono la coerenza temporale e armonica del segnale, ma differiscono nell'intervento. Il **Modello A** è più conservativo, rimuove rumore additivo e trame periodiche mantenendo l’ariosità del suono. Il **Modello B** risulta più aggressivo, riduce distorsioni e saturazioni, ma con rischio di over-smoothing nelle alte frequenze.\n",
        "\n",
        "\n",
        "Poiché i risultati visivi e sonori possono variare a seconda dell’audio generato, le differenze osservate devono essere interpretate in termini qualitativi e di tendenza, non come valori assoluti.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zEGveGzTQIDi"
      }
    }
  ]
}