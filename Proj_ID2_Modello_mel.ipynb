{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jL7Fq75GFiWg",
        "m4_2N48bFkXo",
        "FzF2gThmF7YC"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import librerie"
      ],
      "metadata": {
        "id": "R9GeDARYE6SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marika-rago/projectID2ML\n",
        "%cd projectID2ML"
      ],
      "metadata": {
        "id": "LB06L7mrKTuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import pprint\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import amp\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "metadata": {
        "id": "u6ObEQKDsU_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", message=\".*TorchCodec.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*StreamingMediaDecoder.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*torch.hann_window.*\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"librosa\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"FFMPEG_LOG_LEVEL\"]=\"quiet\"\n"
      ],
      "metadata": {
        "id": "Z0YZG-cosbDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Riproducibilità"
      ],
      "metadata": {
        "id": "E3FWOdThHrVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiamo i parametri di configurazione principali e fissiamo un `seed` globale per la riproducibilità. Questo permette di avere risultati coerenti tra le varie esecuzioni.\n",
        "\n",
        "Nel dizionario `CONFIG` ci sono gli iperparametri comuni a tutta la pipeline, tra cui:\n",
        "- **Frequenza di campionamento** (`sample_rate`)\n",
        "- **Numero di bande Mel** (`n_mels`)\n",
        "- **Parametri della STFT** (`n_fft`, `hop_length`, `win_length`, `window`)  \n",
        "- **Parametri di normalizzazione** (`top_db`, `epsilon`)  \n",
        "- **Frazione del dataset** utilizzata per l’esperimento  \n",
        "- **Seed** per la riproducibilità  \n",
        "\n",
        "Alla fine selezioniamo automaticamente il dispositivo di calcolo (GPU se disponibile).\n"
      ],
      "metadata": {
        "id": "xrG4CaU4bZ3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Riproducibilità\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed) # seed per random\n",
        "    np.random.seed(seed) # seed per NumPy\n",
        "    torch.manual_seed(seed) # seed per PyTorch (CPU)\n",
        "    torch.cuda.manual_seed_all(seed) # seed per cuda\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Seed globale impostato a {seed}\")"
      ],
      "metadata": {
        "id": "Bhj8-HXUsdXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">**ATTENZIONE!!!**\n",
        ">\n",
        ">All'interno del dizionario CONFIG è presente la voce `skip_training`. Se si >vuole eseguire il training cambiare la voce in `False` (di base è settata a `True`).\n",
        "\n"
      ],
      "metadata": {
        "id": "8GuOFwmCMI-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"sample_rate\": 32000,     # Frequenza di campionamento audio\n",
        "    \"n_mels\": 128,            # Numero di bande nel Mel spectrogram\n",
        "    \"n_fft\": 1024,            # Dimensione della FFT\n",
        "    \"hop_length\": 256,        # Passo tra due finestre consecutive\n",
        "    \"win_length\": 1024,       # Lunghezza effettiva della finestra di analisi\n",
        "    \"window\": \"hann\",         # Tipo di finestra (Hann per ridurre le discontinuità)\n",
        "    \"center\": True,           # Centra ogni frame rispetto al segnale originale\n",
        "    \"top_db\": 80.0,           # Range dinamico per la conversione in dB\n",
        "    \"epsilon\": 1e-8,          # Termine di sicurezza per evitare divisioni per zero\n",
        "    \"seed\": 42,               # Seed di riproducibilità\n",
        "    \"dataset_fraction\": 0.16, # Percentuale del dataset usata\n",
        "    \"skip_training\": True,    # Salto del training loop se è true\n",
        "}\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])"
      ],
      "metadata": {
        "id": "keOFV9Tlse7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "maRpFhvPshp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "PG09OlQZFDUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per il training e la valutazione del mio modello è stato utilizzato il dataset **FMA**, in particolare la versiona *fma_small*, che contiene $8.000$ clip da $30$ secondi ciascuna in formati $.mp3$.\n",
        "\n",
        "Quello che facciamo nelle seguenti celle è:\n",
        "\n",
        "1.   scaricare e decomprimere il dataset FMA;\n",
        "2.   contare il numero di file $.mp3$ per capire quanti utilizzarne;\n",
        "3.   selezionare casualmente una porzione del dataset, definita con il parametro `CONFIG[\"dataset_fraction\"]`;\n",
        "4.   dividiamo i file selezionati in tre sottoinsimi, train_files, val_files e test_files;\n",
        "5.   salviamo sul disco la lista dei file selezionati.   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jA4AOXfffAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget https://os.unil.cloud.switch.ch/fma/fma_small.zip\n",
        "!unzip fma_small.zip"
      ],
      "metadata": {
        "id": "b13dUMjLsnm8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/fma_small\"  # cartella che contiene il dataset compresso\n",
        "\n",
        "# Conta tutti i file .mp3 nelle sottocartelle\n",
        "count = sum(len([f for f in files if f.endswith(\".mp3\")]) for _, _, files in os.walk(root_dir))\n",
        "\n",
        "print(f\"Numero totale di file MP3: {count}\")"
      ],
      "metadata": {
        "id": "ONGaVSSHspMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = CONFIG[\"seed\"]\n",
        "fraction = CONFIG[\"dataset_fraction\"]\n",
        "\n",
        "# Imposta il seed per la riproducibilità\n",
        "random.seed(seed)\n",
        "\n",
        "selected_files = []\n",
        "\n",
        "# Scorri tutte le sottocartelle e seleziona un tot di file per ciascuna\n",
        "for subdir, _, files in os.walk(root_dir):\n",
        "    mp3_files = [os.path.join(subdir, f) for f in files if f.endswith(\".mp3\")]\n",
        "    if not mp3_files: # Salta le cartelle vuote\n",
        "        continue\n",
        "    n_select = max(1, int(len(mp3_files) * fraction))\n",
        "    chosen = random.sample(mp3_files, n_select)\n",
        "    selected_files.extend(chosen)\n",
        "\n",
        "print(f\"Totale file selezionati: {len(selected_files)} ({fraction*100:.0f}% del dataset)\")\n",
        "\n",
        "# Salva la lista su file\n",
        "with open(\"selected_files.txt\", \"w\") as f:\n",
        "    for path in selected_files:\n",
        "        f.write(path + \"\\n\")\n",
        "\n",
        "print(\"Lista salvata in 'selected_files.txt'\")"
      ],
      "metadata": {
        "id": "5WvVENYssq5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mischia la lista in modo riproducibile\n",
        "random.seed(42)\n",
        "random.shuffle(selected_files)\n",
        "\n",
        "# Suddivisione (907 + 150 + 150 = 1207)\n",
        "train_files = selected_files[:907]\n",
        "val_files = selected_files[907:1057]\n",
        "test_files = selected_files[1057:]\n",
        "\n",
        "print(f\"Train files: {len(train_files)}\")\n",
        "print(f\"Val files: {len(val_files)}\")\n",
        "print(f\"Test files: {len(test_files)}\")\n"
      ],
      "metadata": {
        "id": "KkVyB4-astD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Degradazioni"
      ],
      "metadata": {
        "id": "uCeMUBHYFGmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe `AudioDegradationDataset` va a definire il dataset che poi utilizziamo per addestrare il modello.\n",
        "L'obiettivo di questa classe è simulare vari tipi di degradazioni audio realistiche a partire dalle clip pulite del datset FMA. In questo modo avremo coppie *(degraded, clean)* su cui il modello deve imparare a ricostruire l'audio eliminando gli artefatti.\n",
        "\n",
        "Le degradazioni vengono applicate dinamicamente ad ogni clip, aumentando così la variabilità dei dati.\n",
        "\n",
        "---\n",
        "\n",
        "**Struttura della Classe**\n",
        "\n",
        "\n",
        "1. *Caricamento e segmentazione* :\n",
        "  ogni clip viene caricata a $32 kHz$ e ridotta a $3$ secondi, in modo da avere input di lunghezza breve e non appesantire il training. Se la clip è più corta da $3$ secondi viene riempita con zeri (padded). Inoltre i segmenti vengono normalizzati.\n",
        "\n",
        "2. *identity_prob* :\n",
        "  con una probabilità del $15\\%$ la clip non viene degradata e rimane uguale a clean. Questo serve per far capire al modello che non deve alterare segnali che sono già buoni di partenza.\n",
        "\n",
        "3. *Degradazioni* :\n",
        "  il metodo `degradation` applica casualmente una delle degradazioni specificate in `degradation_types`. Le varie degradazioni che possono essere applicate sono:\n",
        "\n",
        "      * **Quantizzazione (`quantize`)**: simula la riduzione delle profondità di bit, come avviene nei formati compressi. L'audio viene scalato e arrotondato a una risoluzione di 6, 8 o 10 bit, viene inoltre aggiunto un piccolo `dither` per evitare quantizzazione troppo regolare.\n",
        "\n",
        "      * **Low-pass (`lowpass`)**: simula la perdita delle alte frequenze, l'effetto è che il suono risulta più \"ovattato\". Applica un filtro passa-basso con frequenza di taglio casuale tra $2.5$ e $7$  $kHz$.\n",
        "\n",
        "      * **Clipping (`clipping`)**: simila la distorsione da saturazione del segnale. Vengono troncati tutti i valori oltre una soglia casuale tra $0.6$ e $0.9$.\n",
        "\n",
        "      * **Rumore (`noise`)**: simula un rumore di fondo bianco o rosa, l'effetto è un \"fruscio\" costante.\n",
        "\n",
        "      * **Reverbero (`reverb`)**: simula l'effetto di una stanza. Viene creato una specie di eco con decadimento esponenziale.\n",
        "\n",
        "      * **Distorsione armonica (`distort`)**: simula l'effetto di saturazione dei dispositivi analogici. Applica una non linearita, `tanh`.\n",
        "\n",
        "      * **Tonal Stripes (`tonal_stripes`)**: simula inferenza sinusoidali periodiche. Genera alcune sinusoidi a frequenze casuali (tra $200$ e $800$ $Hz$) e le somma al segnale con ampiezza limitata.\n",
        "\n",
        "4. *Conversione in spettrogrammi Mel* :  \n",
        "  dopo la degradazione il segnale pulito e il segnale degradato vengono convertiti in spettrogrammi Mel tramite `librosa.feature.melspectrogram`. I valori in ampiezza vengono trasformati in decibel e normalizzati in $[0, 1]$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J2y4sgsTk9jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDegradationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, audio_files, sample_rate=32000, segment_length=2.0,\n",
        "                 degradation_types=['quantize', 'lowpass', 'noise', 'tonal_stripes'],\n",
        "                 identity_prob=0.15,\n",
        "                 deterministic=False,\n",
        "                 seed=42):\n",
        "        self.audio_files = audio_files\n",
        "        self.sr = sample_rate\n",
        "        self.segment_samples = int(segment_length * sample_rate)\n",
        "        self.degradation_types = degradation_types\n",
        "        self.identity_prob = identity_prob\n",
        "        self.deterministic = deterministic # Se True blocca la randomizzazione\n",
        "        self.seed = seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "\n",
        "    # Tonal stripes\n",
        "    def add_tonal_stripes(self, audio, sr, num_tones=3, amp_range=(0.02, 0.05),\n",
        "                          min_freq=200, max_freq=8000):\n",
        "        n = len(audio)\n",
        "        t = np.linspace(0, n / sr, n, endpoint=False)\n",
        "        audio = audio.copy()\n",
        "\n",
        "        for _ in range(random.randint(1, num_tones)):\n",
        "            freq = random.uniform(min_freq, max_freq)\n",
        "            amp = random.uniform(*amp_range)\n",
        "            stripe = np.sin(2 * np.pi * freq * t)\n",
        "\n",
        "            # Applica dissolvenze casuali\n",
        "            if random.random() < 0.5:\n",
        "                fade_len = random.randint(int(0.2*n), int(0.6*n))\n",
        "                start = random.randint(0, n - fade_len)\n",
        "                fade = np.linspace(0, 1, fade_len)\n",
        "                stripe[start:start+fade_len] *= fade[::-1] if random.random() < 0.5 else fade\n",
        "\n",
        "            audio += amp * stripe\n",
        "\n",
        "        # Evita saturazione\n",
        "        return np.clip(audio, -1.0, 1.0)\n",
        "\n",
        "    # Degradazione\n",
        "    def degradation(self, audio):\n",
        "        x = audio.copy()\n",
        "\n",
        "        # Scelta casuale\n",
        "        degradation = random.choice(self.degradation_types)\n",
        "\n",
        "        # Quantizzazione\n",
        "        if degradation == 'quantize':\n",
        "            bits = np.random.choice([6, 8, 10])\n",
        "            q = 2 ** bits\n",
        "            dither = np.random.uniform(-1/q, 1/q, size=x.shape)\n",
        "            x = np.round(x * q) / q + dither\n",
        "\n",
        "        # Low pass\n",
        "        elif degradation == 'lowpass':\n",
        "            cutoff = float(np.random.uniform(2500, 7000))\n",
        "            xt = torch.from_numpy(x).unsqueeze(0)\n",
        "            xt = torchaudio.functional.lowpass_biquad(xt, self.sr, cutoff)\n",
        "            x = xt.squeeze(0).numpy()\n",
        "\n",
        "        # Clipping\n",
        "        elif degradation == 'clipping':\n",
        "            thr = float(np.random.uniform(0.6, 0.9))\n",
        "            x = np.clip(x, -thr, thr)\n",
        "\n",
        "        # Rumore bianco o rosa\n",
        "        elif degradation == 'noise':\n",
        "            if np.random.rand() < 0.5:\n",
        "                std = np.random.uniform(0.01, 0.05)\n",
        "                x += np.random.randn(len(x)) * std\n",
        "            else:\n",
        "                white = np.random.randn(len(x))\n",
        "                b = np.cumsum(white)\n",
        "                pink = b / np.max(np.abs(b))\n",
        "                std = np.random.uniform(0.01, 0.04)\n",
        "                x += pink * std\n",
        "\n",
        "        # Reverbero\n",
        "        elif degradation == 'reverb':\n",
        "            decay = np.random.uniform(0.3, 0.9)\n",
        "            ir_len = np.random.randint(2000, 6000)\n",
        "            ir = np.exp(-np.linspace(0, decay, ir_len))\n",
        "            x = np.convolve(x, ir, mode='same')\n",
        "            x = x / (np.max(np.abs(x)) + 1e-8)\n",
        "\n",
        "        # Distorsione\n",
        "        elif degradation == 'distort':\n",
        "            gain = np.random.uniform(1.5, 3.0)\n",
        "            x = np.tanh(x * gain)\n",
        "\n",
        "        # Tonal stripes\n",
        "        elif degradation == 'tonal_stripes':\n",
        "            x = self.add_tonal_stripes(x, self.sr)\n",
        "\n",
        "        return np.clip(x, -1.0, 1.0)\n",
        "\n",
        "    # Caricamento di un file audio\n",
        "    def file_load(self, path):\n",
        "        try:\n",
        "            audio, _ = librosa.load(path, sr=self.sr, mono=True)\n",
        "            return audio\n",
        "\n",
        "        except Exception:\n",
        "            try:\n",
        "                from pydub import AudioSegment\n",
        "                audio = AudioSegment.from_file(path)\n",
        "                samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
        "                samples = samples / (np.max(np.abs(samples)) + 1e-8)\n",
        "                return samples\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Errore nel file: {path} — {type(e).__name__}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, idx):\n",
        "        if self.deterministic:\n",
        "            np.random.seed(self.seed + idx)\n",
        "            random.seed(self.seed + idx)\n",
        "\n",
        "        audio = self.file_load(self.audio_files[idx])\n",
        "\n",
        "        # Se non è riuscito a caricare l’audio, passa al successivo\n",
        "        if audio is None:\n",
        "            print(f\"[WARN] Errore nel file: {self.audio_files[idx]}\")\n",
        "            new_idx = (idx + 1) % len(self.audio_files)\n",
        "            return self.__getitem__(new_idx)\n",
        "\n",
        "        # Segmentazione\n",
        "        if len(audio) > self.segment_samples:\n",
        "            start = np.random.randint(0, len(audio) - self.segment_samples)\n",
        "            audio = audio[start:start + self.segment_samples]\n",
        "        else:\n",
        "            audio = np.pad(audio, (0, self.segment_samples - len(audio)))\n",
        "\n",
        "        # Normalizzazione\n",
        "        max_val = np.max(np.abs(audio)) + CONFIG[\"epsilon\"]\n",
        "        audio = audio / max_val\n",
        "\n",
        "        # Identità stocastica\n",
        "        degraded = audio.copy() if np.random.rand() < self.identity_prob else self.degradation(audio)\n",
        "\n",
        "        # Mel-spectrograms\n",
        "        mel_kwargs = dict(\n",
        "            sr=self.sr,\n",
        "            n_fft=CONFIG[\"n_fft\"],\n",
        "            hop_length=CONFIG[\"hop_length\"],\n",
        "            win_length=CONFIG[\"win_length\"],\n",
        "            window=CONFIG[\"window\"],\n",
        "            center=CONFIG[\"center\"],\n",
        "            n_mels=CONFIG[\"n_mels\"],\n",
        "            power=1.0  # usa ampiezza\n",
        "        )\n",
        "\n",
        "        clean_mel = librosa.feature.melspectrogram(y=audio, **mel_kwargs)\n",
        "        degraded_mel = librosa.feature.melspectrogram(y=degraded, **mel_kwargs)\n",
        "\n",
        "        # Conversione log-dB\n",
        "        clean_mel_db = librosa.amplitude_to_db(np.maximum(clean_mel, CONFIG[\"epsilon\"]),\n",
        "                                               ref=np.max, top_db=CONFIG[\"top_db\"])\n",
        "        degraded_mel_db = librosa.amplitude_to_db(np.maximum(degraded_mel, CONFIG[\"epsilon\"]),\n",
        "                                                  ref=np.max, top_db=CONFIG[\"top_db\"])\n",
        "\n",
        "        # Normalizzazione [0,1]\n",
        "        clean_mel_01 = np.clip((clean_mel_db + CONFIG[\"top_db\"]) / CONFIG[\"top_db\"], 0.0, 1.0)\n",
        "        degraded_mel_01 = np.clip((degraded_mel_db + CONFIG[\"top_db\"]) / CONFIG[\"top_db\"], 0.0, 1.0)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(degraded_mel_01).unsqueeze(0).float(),\n",
        "            torch.from_numpy(clean_mel_01).unsqueeze(0).float()\n",
        "        )\n"
      ],
      "metadata": {
        "id": "Q2N0oNJae25k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gruppi di degradazioni\n",
        "gruppo_A = ['quantize', 'tonalstripes', 'noise']\n",
        "gruppo_B = ['clipping', 'reverb', 'lowpass', 'distort']"
      ],
      "metadata": {
        "id": "ElhBQSBHs1xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Gruppo A"
      ],
      "metadata": {
        "id": "Rd2QcIH-FOWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Richiamiamo la classe AudioDegradationDataset sulle degradazioni del gruppo A\n",
        "train_dataset_A = AudioDegradationDataset(\n",
        "    train_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=False\n",
        ")\n",
        "val_dataset_A = AudioDegradationDataset(\n",
        "    val_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=True,\n",
        "    seed=23\n",
        ")\n",
        "test_dataset_A = AudioDegradationDataset(\n",
        "    test_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_A,\n",
        "    deterministic=True,\n",
        "    seed=19\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset_A)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_A)}\")\n",
        "print(f\"Test samples: {len(test_dataset_A)}\")\n"
      ],
      "metadata": {
        "id": "lC7CqzPYs3wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Gruppo B"
      ],
      "metadata": {
        "id": "w9xx-nNyFUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Richiamiamo la classe AudioDegradationDataset sulle degradazioni del gruppo B\n",
        "train_dataset_B = AudioDegradationDataset(\n",
        "    train_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=False\n",
        ")\n",
        "val_dataset_B = AudioDegradationDataset(\n",
        "    val_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=True,\n",
        "    seed=23\n",
        ")\n",
        "test_dataset_B = AudioDegradationDataset(\n",
        "    test_files,\n",
        "    sample_rate=32000,\n",
        "    segment_length=3.0,\n",
        "    degradation_types=gruppo_B,\n",
        "    deterministic=True,\n",
        "    seed=19\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset_B)}\")\n",
        "print(f\"Validation samples: {len(val_dataset_B)}\")\n",
        "print(f\"Test samples: {len(test_dataset_B)}\")\n"
      ],
      "metadata": {
        "id": "efAwU557s5nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizzazione esempio"
      ],
      "metadata": {
        "id": "tkF8DPWmIwu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funzione `show_example_spectrogram` visualizziamo una coppia di spettrogrammi (clean e degraded) in modo da vedere come viene modificata la clip.\n",
        "\n",
        "Ogni spettrogramma è una rappresentazione tempo-frequenza. Sull'asse delle $x$ ci sono i frame temporali, sull'asse delle $y$ ci sono le bande Mel."
      ],
      "metadata": {
        "id": "XHOrTWgOxeb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example_spectrogram(dataset, idx=0):\n",
        "\n",
        "    # Estrae la coppia (degraded, clean)\n",
        "    degraded_mel, clean_mel = dataset[idx]\n",
        "\n",
        "    # Rimuove la dimensione del canale [1, F, T] --> [F, T]\n",
        "    clean_mel = clean_mel.squeeze().numpy()\n",
        "    degraded_mel = degraded_mel.squeeze().numpy()\n",
        "\n",
        "    print(f\"Shape clean_mel: {clean_mel.shape} | Shape degraded_mel: {degraded_mel.shape}\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    # Plot clean\n",
        "    im1 = axes[0].imshow(clean_mel, aspect='auto', origin='lower')\n",
        "    axes[0].set_title('Clean Mel-Spectrogram (dB --> [0,1])')\n",
        "    axes[0].set_ylabel('Mel Frequency')\n",
        "    fig.colorbar(im1, ax=axes[0])\n",
        "\n",
        "    # Plot degraded\n",
        "    im2 = axes[1].imshow(degraded_mel, aspect='auto', origin='lower')\n",
        "    axes[1].set_title('Degraded Mel-Spectrogram (dB → [0,1])')\n",
        "    axes[1].set_ylabel('Mel Frequency')\n",
        "    axes[1].set_xlabel('Time Frames')\n",
        "    fig.colorbar(im2, ax=axes[1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vmsQjCG4I6gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio Gruppo A"
      ],
      "metadata": {
        "id": "Q839DWm6J5dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_example_spectrogram(train_dataset_A, idx=15)"
      ],
      "metadata": {
        "id": "winaSs1nJ8pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio Gruppo B"
      ],
      "metadata": {
        "id": "Fkh1DQ93J_bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_example_spectrogram(train_dataset_B, idx=15)"
      ],
      "metadata": {
        "id": "osnJzl7rJ_bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "OTtVmAFpFZcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dopo la definizione dei dataset per i gruppi $A$ e $B$, definiamo ora i corrispondenti `DataLoader`, che gestiscono il caricamento e la preparazione dei batch per il training e la validazione.\n",
        "\n",
        "---\n",
        "\n",
        "Settiamo i parametri generali con `LOADER_CONFIG`:\n",
        "\n",
        "* `batch_size` = $16$, è il numero di esempi per batch.\n",
        "\n",
        "* `num_workers` = $0$, lo impostiamo a $0$ per compatibilità con Colab evitando di saturare la RAM.\n",
        "\n",
        "* `pin_memory` = $True$, blocca la memoria dei batch in RAM, in modo da trasferirli più velocemente sulla GPU.\n",
        "\n",
        "* `persistent_workers` = $False$, evita che i processi di caricamento restino attivi dopo ogni epoca.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abcDWg26z61t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri generali\n",
        "LOADER_CONFIG = {\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 0,\n",
        "    \"pin_memory\": True,\n",
        "    \"persistent_workers\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "A33JE20ptcWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Gruppo A"
      ],
      "metadata": {
        "id": "b6LGOwsIFcoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_A = DataLoader(\n",
        "    train_dataset_A,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=True, # Mescola i campioni ad ogni epoca\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "val_loader_A = DataLoader(\n",
        "    val_dataset_A,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=False, # non mescola in modo da vere una validazione deterministica\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "test_loader_A = DataLoader(\n",
        "    test_dataset_A,\n",
        "    batch_size=1, # analizza un file per volta\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader_A)}, Val batches: {len(val_loader_A)}, Test batches: {len(test_loader_A)}\")"
      ],
      "metadata": {
        "id": "h9E1oDGBtfTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Gruppo B"
      ],
      "metadata": {
        "id": "5EcqrleDFfbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_B = DataLoader(\n",
        "    train_dataset_B,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "val_loader_B = DataLoader(\n",
        "    val_dataset_B,\n",
        "    batch_size=LOADER_CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=LOADER_CONFIG[\"num_workers\"],\n",
        "    pin_memory=LOADER_CONFIG[\"pin_memory\"],\n",
        "    persistent_workers=LOADER_CONFIG[\"persistent_workers\"]\n",
        ")\n",
        "\n",
        "test_loader_B = DataLoader(\n",
        "    test_dataset_B,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader_B)}, Val batches: {len(val_loader_B)}, Test batches: {len(test_loader_B)}\")"
      ],
      "metadata": {
        "id": "gOmRnh0NtjXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello"
      ],
      "metadata": {
        "id": "jL7Fq75GFiWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il mio modello prende in input uno spettrogramma normalizzato in $[0,1]$ (lineare o Mel, 1 canale) e produce uno spettrogramma \"enhanced\" della stessa forma.\n",
        "Segue lo stile di una U-Net, e integra un contesto multi-scala.\n",
        "\n",
        "---\n",
        "\n",
        "La funzione `crop_to_match` serve per fare combaciare i due tensori che prende in input. Serve perchè le operazioni di downsampling e upsampling possono generare piccole differenze di pixel, e prima di concatenare le skip connections occorre che le dimensioni siano uguali.\n",
        "\n",
        "---\n",
        "\n",
        "La calsse `MultiScaleCNNBlock` applica tre convoluzioni 2d in parallelo con kernel $3 \\times 3$, $5 \\times 5$, $7 \\times 7$. Successivamente fa una fusione con conv $1 \\times 1$ e poi facoltativamente applica ReLU.\n",
        "\n",
        "Tutto questo viene fatto perchè gli artefatti possono essere locali o più larghi.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `ResidualBlock` applica due conv $3 \\times 3$ con BatchNorm e skip residual, e infine applica ReLU. Questo viene fatto per preservare informazioni ed evitare vanishing gradient.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `UpBlock` fa upsampling bilineare seguita da conv $3 \\times 3$, BatchNorm e infine ReLU.\n",
        "\n",
        "---\n",
        "\n",
        "La classe `SpctralEnhancementNet` è composta da tre parti principali: Encoder, Bottleneck e Decoder.\n",
        "\n",
        "\n",
        "L'**Encoder** riduce progressivamente la risoluzione spaziale dello spettrogramma. Ogni blocco MultiScakeCNNBlock estrae feature locali e globali come detto precedentemente. Dopo ogni blocco, una convoluzione con stride $2$ dimezza le simesioni dello spettro, comprimendo l'informazione e ampliando il campo percettivo. Qui la rete impara a vedere il quadro complessivo del segnale.\n",
        "\n",
        "\n",
        "Nella parte centrale, **Bottleneck**, tre blocchi residiali lavorano mantenendo la profondità costante.\n",
        "Questi blocchi permettono di rielaborare le caratteristiche apprese mantenendo l’informazione originale, grazie alle skip connection tra input e output del blocco.\n",
        "Questo meccanismo stabilizza l’addestramento e migliora la capacità della rete di affinare dettagli senza distruggere la struttura armonica del segnale.\n",
        "Viene applicato anche un Dropout2D per ridurre l’overfitting.\n",
        "\n",
        "\n",
        "Il **Decoder** ricostruisce progressivamente la risoluzione originale dello spettrogramma, invertendo il processo di compressione.\n",
        "Ogni livello effettua un upsampling bilineare seguito da una convoluzione $3 \\times 3$ e da una fusione con le feature corrispondenti dell’encoder.\n",
        "Le skip connections collegano i livelli simmetrici dell’encoder e del decoder, in modo che la rete possa recuperare dettagli fini persi durante il downsampling.\n",
        "Dopo la concatenazione, un nuovo MultiScaleCNNBlock elabora le informazioni combinate, permettendo alla rete di fondere dettagli locali e contesto globale.\n",
        "\n",
        "\n",
        "Infine, un’uscita convoluzionale $1 \\times 1$ riduce i canali a uno solo, producendo il mel-spettrogramma enhanced.\n",
        "L’attivazione finale è lineare, il clamp a $[0, 1]$ viene applicato solo durante il training e il test per coerenza con la normalizzazione dei dati.\n",
        "\n"
      ],
      "metadata": {
        "id": "nBDe-xTz26g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_to_match(source, target):\n",
        "\n",
        "    _, _, h, w = source.shape\n",
        "    _, _, h_t, w_t = target.shape\n",
        "\n",
        "    # Se le dimensioni non coincidono, le ritaglia a quella minima\n",
        "    if h != h_t or w != w_t:\n",
        "        h_min = min(h, h_t)\n",
        "        w_min = min(w, w_t)\n",
        "        source = source[:, :, :h_min, :w_min]\n",
        "        target = target[:, :, :h_min, :w_min]\n",
        "    return source, target"
      ],
      "metadata": {
        "id": "KMUlSEqgtrNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleCNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, last_relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Suddivide i canali di outout tra i tre rami\n",
        "        base = out_channels // 3\n",
        "        rem = out_channels - 3 * base\n",
        "        ch1, ch2, ch3 = base, base, base + rem  # branch3 assorbe il resto\n",
        "\n",
        "        # Ramo 1 kernel 3x3\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch1, kernel_size=3, padding=1, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Ramo 2 kernel 5x5\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch2, kernel_size=5, padding=2, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Ramo 3 kernel 7x7\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, ch3, kernel_size=7, padding=3, padding_mode='reflect'),\n",
        "            nn.BatchNorm2d(ch3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Dopo concatenazione riduce i canali con con 1x1\n",
        "        fused_in = ch1 + ch2 + ch3\n",
        "        layers = [nn.Conv2d(fused_in, out_channels, kernel_size=1)]\n",
        "        if last_relu:\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        self.fusion = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Calcola le tre rappresentazioni parallele\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "\n",
        "        # Concatena lungo la dimensione dei canali\n",
        "        out = torch.cat([b1, b2, b3], dim=1)\n",
        "\n",
        "        # Fusione e riduzione dei canali\n",
        "        return self.fusion(out)"
      ],
      "metadata": {
        "id": "HoJw0Se0ttVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect')\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, padding_mode='reflect')\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x # Skip Connection\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + residual # Somma residua\n",
        "        return F.relu(out)"
      ],
      "metadata": {
        "id": "c2EuSzX5tvE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, padding_mode='reflect')\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x) # Upsampling (fa interpolazione bilineare)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4ZFSLfDqogWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralEnhancementNet(nn.Module):\n",
        "    def __init__(self, n_mels=128):\n",
        "        super().__init__()\n",
        "        self.n_mels = n_mels # Utile per mel ma non usato direttamente\n",
        "\n",
        "        # Encoder --> estrae feature\n",
        "        self.enc1 = MultiScaleCNNBlock(1, 32)\n",
        "        self.down1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        self.enc2 = MultiScaleCNNBlock(32, 64)\n",
        "        self.down2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        self.enc3 = MultiScaleCNNBlock(64, 128)\n",
        "        self.down3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, padding_mode='reflect')\n",
        "\n",
        "        # Bottleneck --> 3 blocchi residuali\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "        )\n",
        "\n",
        "        # Decoder --> upsampling progressivo e concatenazione con feture encoder\n",
        "        self.up3 = UpBlock(128, 64)\n",
        "        self.dec3 = MultiScaleCNNBlock(64 + 128, 64)\n",
        "\n",
        "        self.up2 = UpBlock(64, 32)\n",
        "        self.dec2 = MultiScaleCNNBlock(32 + 64, 32)\n",
        "\n",
        "        self.up1 = UpBlock(32, 16)\n",
        "        self.dec1 = MultiScaleCNNBlock(16 + 32, 16, last_relu=False)\n",
        "\n",
        "        # Output --> spettrogramma migliorato (un canale)\n",
        "        self.out = nn.Conv2d(16, 1, kernel_size=1)\n",
        "        self.dropout = nn.Dropout2d(p=0.1) # Regolarizzazione nel bottleneck\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e1d = self.down1(e1)\n",
        "        e2 = self.enc2(e1d)\n",
        "        e2d = self.down2(e2)\n",
        "        e3 = self.enc3(e2d)\n",
        "        e3d = self.down3(e3)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.res_blocks(e3d)\n",
        "        b = self.dropout(b)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.up3(b)\n",
        "        d3, e3 = crop_to_match(d3, e3)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2, e2 = crop_to_match(d2, e2)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1, e1 = crop_to_match(d1, e1)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        # Output\n",
        "        enhanced = torch.sigmoid(self.out(d1))\n",
        "        return enhanced"
      ],
      "metadata": {
        "id": "LNqM2D3Vtwwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "m4_2N48bFkXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La loss che utilizzo nel mio modello\n",
        "`HybridLoss` combina diverse componenti, ognuna della quali si occupa di un aspetto diverso della qualità audio.\n",
        "\n",
        "La loss totale è una somma pesata dei vari termini:\n",
        "\n",
        "$$L_{tot} = w_c \\cdot L_{charb} + w_{hf} \\cdot L_{hf} + w_{dnh} \\cdot L_{dnh} + w_{f} \\cdot L_{flat} + w_{bw} \\cdot L_{bw} + w_e \\cdot L_{energy} $$\n",
        "\n",
        "Dove:\n",
        "\n",
        "* **Charbonnier Loss**: penalizza le differenze punto per punto tra lo spettrogramma predetto ($x$) e il target ($y$). È robusta agli outliers.\n",
        "\n",
        "$$L_{charb} = \\frac{1}{N} \\sum_{f,t}\n",
        "\\sqrt{(x_{f,t} - y_{f,t})^2 + \\varepsilon^2}$$\n",
        "\n",
        "* **High Frequency Loss**: enfatizza le regioni ad alta frequenza e ad alta energia, poichè mi sono accorta che il modello tendeva a smussare nella zona alta.\n",
        "\n",
        "$$L_{hf} = \\frac{1}{N} \\sum_{f,t}\n",
        "w(f) \\cdot m_{f,t} \\, |x_{f,t} - y_{f,t}|$$\n",
        "\n",
        "* **Do No Harm Loss**: scoraggia la rete a modificare le zone già \"buone\".\n",
        "\n",
        "$$L_{dnh} = \\frac{1}{N} \\sum_{f,t}\n",
        "(1 - n_{f,t}) \\, |x_{f,t} - d_{f,t}| $$\n",
        "\n",
        "* **Spectral Flatness e Tonal Penalty Loss**: mantiene la coerenza tonale, evitando che l'output risulti \"metallico\" o troppo \"piatto\".\n",
        "\n",
        "$$SF(S) = \\frac{\\exp \\left( \\frac{1}{T} \\sum_{t} \\log(S_{f,t} + \\varepsilon) \\right)}\n",
        "{\\frac{1}{T} \\sum_{t} (S_{f,t} + \\varepsilon)} $$\n",
        "\n",
        "$$L_{flat} = \\frac{1}{F} \\sum_{f}\n",
        "|SF(x_f) - SF(y_f)| $$\n",
        "\n",
        "* **Band Weighted L1 Loss**: rafforza la coerenza nelle bande di frequenza più importanti per l'udito umano.\n",
        "\n",
        "$$L_{bw} = \\frac{1}{N} \\sum_{f,t}\n",
        "w(f) \\, |x_{f,t} - y_{f,t}| $$\n",
        "\n",
        "* **Energy Consistency Loss**: preserva il volume e il bilanciamento energetico del segnale.\n",
        "\n",
        "$$L_{energy} = \\frac{1}{B} \\sum_{b=1}^{B}\n",
        "\\left| E(x_b) - E(y_b) \\right|\n",
        "\\quad \\text{con} \\quad\n",
        "E(S) = \\frac{1}{F T} \\sum_{f,t} S_{f,t}^2 $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eA9Rh0HOc1DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weights=None, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps # Evita divisioni per 0\n",
        "\n",
        "        # Setta i pesi per ogni componente\n",
        "        self.w = {\n",
        "            \"charb\": 1.0,\n",
        "            \"hf\": 0.4,\n",
        "            \"dnh\": 0.25,\n",
        "            \"flat\": 0.05,\n",
        "            \"bw\": 0.4,\n",
        "            \"energy\": 0.1\n",
        "        }\n",
        "        if weights is not None:\n",
        "            self.w.update(weights)\n",
        "\n",
        "\n",
        "    # Charbonnier loss\n",
        "    def charbonnier(self, x, y):\n",
        "        diff = x - y\n",
        "        return torch.mean(torch.sqrt(diff * diff + self.eps * self.eps))\n",
        "\n",
        "\n",
        "    # High-frequency\n",
        "    def hf_guided(self, pred, target):\n",
        "        B, C, F, T = pred.shape\n",
        "        # peso crescente lungo la frequenza\n",
        "        w_f = torch.linspace(0.9, 1.1, F, device=pred.device).view(1, 1, F, 1)\n",
        "        # Normalizzazione del target per generare una maschera percettiva\n",
        "        t_norm = (target - target.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0]) \\\n",
        "                 / (target.max(dim=2, keepdim=True)[0].max(dim=3, keepdim=True)[0] -\n",
        "                    target.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0] + self.eps)\n",
        "        mask = torch.clamp(t_norm, 0.0, 1.0) # [B,1,F,T]\n",
        "        return torch.mean(w_f * mask * torch.abs(pred - target))\n",
        "\n",
        "\n",
        "    # Do-no-harm\n",
        "    def do_no_harm(self, pred, target, degraded, tau=0.05):\n",
        "        # Calcola dove degraded è già simile al target\n",
        "        need = torch.abs(target - degraded)\n",
        "        need = need / (need.amax(dim=(2, 3), keepdim=True) + self.eps)\n",
        "        mask_no_need = 1.0 - need\n",
        "        # Penalizza modifiche inutili per evitare di peggiorare cose buone\n",
        "        return torch.mean(mask_no_need * torch.abs(pred - degraded))\n",
        "\n",
        "\n",
        "    # Spectral flatness penalty\n",
        "    def spectral_flatness(self, S):\n",
        "        # Evita log(0)\n",
        "        S = torch.clamp(S, min=self.eps)\n",
        "        # Media aritmetica e geometrica lungo il tempo\n",
        "        am = S.mean(dim=3)\n",
        "        gm = torch.exp((torch.log(S)).mean(dim=3))\n",
        "        sf = gm / (am + self.eps)\n",
        "        return sf\n",
        "\n",
        "    def tonal_penalty(self, pred, target):\n",
        "        # Differenza di flatness tra predizione e target\n",
        "        sf_p = self.spectral_flatness(pred)\n",
        "        sf_t = self.spectral_flatness(target)\n",
        "        return torch.mean(torch.abs(sf_p - sf_t))\n",
        "\n",
        "\n",
        "    # Band-weighted L1\n",
        "    def band_weighted_l1(self, pred, target):\n",
        "        B, C, F, T = pred.shape\n",
        "        # Curva sinusoidale\n",
        "        weights = 1.0 + 0.15 * torch.sin(torch.linspace(0, 3.14, F, device=pred.device))\n",
        "        weights = weights.view(1, 1, F, 1)\n",
        "        return torch.mean(weights * torch.abs(pred - target))\n",
        "\n",
        "\n",
        "    # Energy consistency\n",
        "    def energy_consistency(self, pred, target):\n",
        "        # Energia media per spettrogramma\n",
        "        e_pred = torch.mean(pred ** 2, dim=(2, 3))\n",
        "        e_tgt = torch.mean(target ** 2, dim=(2, 3))\n",
        "        return torch.mean(torch.abs(e_pred - e_tgt))\n",
        "\n",
        "\n",
        "    # Forward\n",
        "    def forward(self, enhanced, target, degraded=None, return_components=False):\n",
        "        # Componenti principali della loss\n",
        "        charb = self.charbonnier(enhanced, target)\n",
        "        hf = self.hf_guided(enhanced, target)\n",
        "        bw = self.band_weighted_l1(enhanced, target)\n",
        "        energy = self.energy_consistency(enhanced, target)\n",
        "\n",
        "        # Somma pesata delle componenti principali\n",
        "        total = (\n",
        "            self.w[\"charb\"] * charb +\n",
        "            self.w[\"bw\"] * bw +\n",
        "            self.w[\"hf\"] * hf +\n",
        "            self.w[\"energy\"] * energy\n",
        "        )\n",
        "\n",
        "        # Componenti opzionali\n",
        "        dnh = None\n",
        "        if degraded is not None and self.w[\"dnh\"] > 0:\n",
        "            dnh = self.do_no_harm(enhanced, target, degraded)\n",
        "            total += self.w[\"dnh\"] * dnh\n",
        "\n",
        "        flat = None\n",
        "        if self.w[\"flat\"] > 0:\n",
        "            flat = self.tonal_penalty(enhanced, target)\n",
        "            total += self.w[\"flat\"] * flat\n",
        "\n",
        "        # Restituisce anche le singole componenti\n",
        "        if return_components:\n",
        "            out = {\"total\": total, \"charb\": charb, \"bw\": bw, \"hf\": hf, \"energy\": energy}\n",
        "            if dnh is not None: out[\"dnh\"] = dnh\n",
        "            if flat is not None: out[\"flat\"] = flat\n",
        "            return out\n",
        "        else:\n",
        "            return total\n"
      ],
      "metadata": {
        "id": "lepvqn6xt5ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "UzVB4XIhFnSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metriche"
      ],
      "metadata": {
        "id": "N1LN9vFesBQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le metriche permettono di valutare quanto bene il modello sta facendo.\n",
        "Ho utilizzato quattro metriche, ognuna che guarda un aspetto diverso del segnale.\n",
        "\n",
        "---\n",
        "\n",
        "**Mean Squared Error**\n",
        "\n",
        "Misura la distanza quadratica media tra lo spettrogramma predetto $x$ e lo spettrogramma target $y$.\n",
        "\n",
        "Valori più bassi indicano una ricostruzione più fedele.\n",
        "\n",
        "$$MSE = \\frac{1}{N} \\sum_{f, t}\n",
        "(x_{f,t} - y_{f,t})^2 $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**L1, Mean Absolute Error**\n",
        "\n",
        "Indica quanto, in media, ogni punto delle spettrogramma differisce dal corrispondente valore del target. Permette di controllare la precisione media.\n",
        "\n",
        "Valori minori indicano una migliore ricostruzione.\n",
        "\n",
        "$$L1 = \\frac{1}{N} \\sum_{f, t}\n",
        "|x_{f,t} - y_{f,t}|$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Cosine Similarity**\n",
        "\n",
        "Misura l'orientamento tra i vettori. Vale:\n",
        "\n",
        "* $1$ se i due spettrogrammi hanno la stessa forma spettrale, anche se con differente scala;\n",
        "* $0$ se sono ortogonali;\n",
        "* $-1$ se sono opposti.\n",
        "\n",
        "Valori vicino a $1$ indicano una forte similarità strutturale.\n",
        "\n",
        "\n",
        "$$ CosSim = \\frac {\\mathbf{x} \\cdot \\mathbf{y}}\n",
        "{\\|\\mathbf{x}\\|_2 \\, \\|\\mathbf{y}\\|_2}\n",
        "= \\frac{\\sum_{i=1}^{N} x_i y_i}\n",
        "{\\sqrt{\\sum_{i=1}^{N} x_i^2} \\, \\sqrt{\\sum_{i=1}^{N} y_i^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Log Spectral Distance (LSD)**\n",
        "\n",
        "Misura la differenza media tra due spettrogrammi nel dominio logaritmico (in decibel). In altre parole, valuta quanto l'energia percepita uditivamente è simile tra le due tracce.\n",
        "\n",
        "$$LSD = \\sqrt{\\frac{1}{N} \\sum_{k=1}^{N}\n",
        "\\left( 10 \\log_{10}\n",
        "\\frac{|S_{\\text{pred}}(k)|^2}\n",
        "{|S_{\\text{target}}(k)|^2}\n",
        "\\right)^{2}} $$\n",
        "\n",
        "In questo modello basato su spettrogrammi Mel, i valori di input e output sono già normalizzati nel dominio logaritmico (in dB).\n",
        "Pertanto, nella pratica, l’LSD viene calcolata direttamente come Root Mean Square Error (RMSE) tra i valori logaritmici (espressi in dB). Questa formulazione è numericamente equivalente alla precedente, ma evita la doppia conversione logaritmica e risulta più stabile nel caso di spettrogrammi Mel già compressi in scala decibel.\n",
        "\n",
        "$$LSD = \\sqrt{\\frac{1}{N} \\sum_{k=1}^{N}\n",
        "\\left( S_{\\text{pred}}^{(\\mathrm{dB})}(k) - S_{\\text{target}}^{(\\mathrm{dB})}(k) \\right)^2 }$$\n",
        "\n"
      ],
      "metadata": {
        "id": "gQnAxwDTmk1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_spec(pred, target):\n",
        "    return torch.mean((pred - target) ** 2).item()\n",
        "\n",
        "\n",
        "def l1_spec(pred, target):\n",
        "    return torch.mean(torch.abs(pred - target)).item()\n",
        "\n",
        "\n",
        "def cosine_spec(pred, target):\n",
        "    pred_f = pred.flatten(start_dim=1).float()\n",
        "    target_f = target.flatten(start_dim=1).float()\n",
        "\n",
        "    # Centra i vettori (rimuove il bias positivo da [0,1])\n",
        "    pred_f = pred_f - pred_f.mean(dim=1, keepdim=True)\n",
        "    target_f = target_f - target_f.mean(dim=1, keepdim=True)\n",
        "\n",
        "    cos = F.cosine_similarity(pred_f, target_f, dim=1)\n",
        "    cos = torch.clamp(cos, -1.0, 1.0)\n",
        "    return torch.clamp(torch.mean(cos), -1.0, 1.0).item()\n",
        "\n",
        "\n",
        "def lsd_spec(pred, target, top_db=80.0):\n",
        "    # Siamo già in scala logaritmica\n",
        "    pred_db = pred * top_db - top_db\n",
        "    tgt_db = target * top_db - top_db\n",
        "    diff = torch.nan_to_num(pred_db - tgt_db, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return torch.sqrt(torch.mean(diff ** 2)).item()\n"
      ],
      "metadata": {
        "id": "jTq-S5PmuBgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni Ausiliarie"
      ],
      "metadata": {
        "id": "enPSsv5LFpuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante l'elaborazione degli spettrogrammi Mel, è possibile che l'output prodotto dalla rete abbia dimensioni leggermente diverse rispetto al target. Queste discrepanze sono causate dalle convoluzioni, downsampling e upsampling, che possono modificare l'altezza o la larghezza del tensore.\n",
        "\n",
        "Per evitare errori quando calcoliamo la loss e le metriche, è necessario che la predizione del modello e il target abbiamo esattamente la stessa forma.\n",
        "\n",
        "La funzione `match_shape` uniforma le dimensioni dei due tensori. Confronta altezza e larghezza della predizione e del target. Se la dimensione della predizione è maggiore la taglia (crop), se è minore la riempie (pad) con zeri fino ad uguagliarla.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GfNvKhD8wYKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_shape(pred, target):\n",
        "    # Estrae le dimensioni di pred e target\n",
        "    _, _, h_p, w_p = pred.shape\n",
        "    _, _, h_t, w_t = target.shape\n",
        "\n",
        "    # Allinea l'altezza (frequenze)\n",
        "    if h_p > h_t:\n",
        "        pred = pred[:, :, :h_t, :]\n",
        "    elif h_p < h_t:\n",
        "        pred = F.pad(pred, (0, 0, 0, h_t - h_p))\n",
        "\n",
        "    # Allinea la larghezza (tempo)\n",
        "    if w_p > w_t:\n",
        "        pred = pred[:, :, :, :w_t]\n",
        "    elif w_p < w_t:\n",
        "        pred = F.pad(pred, (0, w_t - w_p, 0, 0))\n",
        "\n",
        "    return pred, target\n"
      ],
      "metadata": {
        "id": "IDNHIEABuKPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "XQ4AgrYGFs3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione `train_spectral_model` implementa il ciclo di addestramento completo del modello di spettrogramma Mel.\n",
        "\n",
        "La funzione è divisa in varie sezioni:\n",
        "\n",
        "1. **Inizializzazione**: mandiamo il modello sul dispositivo (`cuda` se disponibile altrimenti `cpu`). Creiamo un `GradScaler` per utilizzare mixed precision training, e inizializzaimo le variabili che conterranno le varie train loss, val loss e i valori delle metriche.\n",
        "\n",
        "2. **Loop per ogni epoca**: in ogni  epoca distinguiamo due fasi:\n",
        "    * **Training**: mettiamo il modello in modalità `train`. Per ogni batch calcoliamo la predizione (`enhanced`), la loss e aggiornaimo i pesi del modello.\n",
        "    * **Validation**: mettiamo il modello in modalità `eval`, calcoliamo la loss e le metriche. Confrontiamo i risultati tra `degraded` ed `enhanced` per valutare il miglioramento medio (`delta`).\n",
        "\n",
        "3. **Aggiormaneto dello scheduler**: se è definito un learning rate scheduler lo aggiorniamo in base alla validation loss.\n",
        "\n",
        "4. **Early Stopping e salvataggio Checkpoint**: se la `val_loss` migliora il modello viene salvato. Se il modello non migliora per `patience` epoche consecutive l'addestramenti si interrompe.\n",
        "\n",
        "5. **Visualizzazione plot**: se `show_example=True` vengono mostrati gli spettrogrammi clean, degraded ed enhanced. Se `show_metrics=True` vengono mostrati i valori delle metriche per valutare i progressi."
      ],
      "metadata": {
        "id": "Wn_N0i3uz_kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spectral_model(\n",
        "    model,\n",
        "    train_dl,\n",
        "    val_dl,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler=None,\n",
        "    num_epochs=20,\n",
        "    patience=5,\n",
        "    device=\"cuda\",\n",
        "    save_path=\"/content/checkpoint_best.pt\",\n",
        "    show_example=False, # Per mostrare gli spettrogrammi\n",
        "    show_metrics=True, # Per mostrare le metriche\n",
        "    use_amp=True,\n",
        "    log_loss_components=False # Per mostrare i valori delle varie componenti della loss\n",
        "):\n",
        "\n",
        "    model = model.to(device)\n",
        "    scaler = amp.GradScaler('cuda', enabled=use_amp)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    train_losses, val_losses, val_metrics_list = [], [], []\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n----- Epoch {epoch}/{num_epochs} -----\")\n",
        "\n",
        "\n",
        "        # TRAINING\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_bar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n",
        "\n",
        "        for mel_degraded, mel_clean in train_bar:\n",
        "            # Sposta su device\n",
        "            mel_degraded = mel_degraded.to(device, non_blocking=True)\n",
        "            mel_clean = mel_clean.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Forward\n",
        "            with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                enhanced = model(mel_degraded)\n",
        "\n",
        "                # Allinea le dimensioni\n",
        "                enhanced, mel_clean = crop_to_match(enhanced, mel_clean)\n",
        "                mel_degraded, _ = crop_to_match(mel_degraded, mel_clean)\n",
        "\n",
        "                mel_clean_n = mel_clean\n",
        "                mel_degraded_n = mel_degraded\n",
        "                enhanced_n = enhanced\n",
        "                enhanced_n = torch.clamp(enhanced_n, 0.0, 1.0)\n",
        "\n",
        "                # Calcolo loss\n",
        "                loss = loss_fn(enhanced_n, mel_clean_n, mel_degraded_n)\n",
        "\n",
        "            # Controlla la stabilità numerica\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\"NaN/Inf rilevato — training interrotto.\")\n",
        "                return model, train_losses, val_losses, val_metrics_list\n",
        "\n",
        "            # Backprop\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.detach().item()\n",
        "            train_bar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= max(1, len(train_dl))\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        metrics_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0, \"LSD\": 0}\n",
        "        delta_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0, \"LSD\": 0}\n",
        "        n_batches = 0\n",
        "\n",
        "        val_bar = tqdm(val_dl, desc=f\"Val {epoch}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for mel_degraded, mel_clean in val_bar:\n",
        "                mel_degraded = mel_degraded.to(device, non_blocking=True)\n",
        "                mel_clean = mel_clean.to(device, non_blocking=True)\n",
        "\n",
        "                with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                    enhanced = model(mel_degraded)\n",
        "                    enhanced, mel_clean = crop_to_match(enhanced, mel_clean)\n",
        "                    mel_degraded, _ = crop_to_match(mel_degraded, mel_clean)\n",
        "\n",
        "\n",
        "                    mel_clean_n = mel_clean\n",
        "                    mel_degraded_n = mel_degraded\n",
        "                    enhanced_n = enhanced\n",
        "                    enhanced_n = torch.clamp(enhanced_n, 0.0, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "                    # Se True prende anche le singole componenti della loss\n",
        "                    if log_loss_components:\n",
        "                        out = loss_fn(enhanced_n, mel_clean_n, degraded=mel_degraded_n, return_components=True)\n",
        "                        loss = out[\"total\"]\n",
        "                        comp = {k: v.item() if torch.is_tensor(v) else v for k, v in out.items()}\n",
        "                    else:\n",
        "                        loss = loss_fn(enhanced_n, mel_clean_n, mel_degraded_n)\n",
        "                        comp = None\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Metriche\n",
        "                restored_metrics = {\n",
        "                    \"MSE\": mse_spec(enhanced_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"L1\": l1_spec(enhanced_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"COS\": cosine_spec(enhanced_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"LSD\": lsd_spec(enhanced_n.cpu(), mel_clean_n.cpu(), CONFIG[\"top_db\"])\n",
        "                }\n",
        "                baseline_metrics = {\n",
        "                    \"MSE\": mse_spec(mel_degraded_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"L1\": l1_spec(mel_degraded_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"COS\": cosine_spec(mel_degraded_n.cpu(), mel_clean_n.cpu()),\n",
        "                    \"LSD\": lsd_spec(mel_degraded_n.cpu(), mel_clean_n.cpu(), CONFIG[\"top_db\"])\n",
        "                }\n",
        "                # Delta\n",
        "                delta_metrics = {k: baseline_metrics[k] - restored_metrics[k] for k in restored_metrics}\n",
        "\n",
        "\n",
        "                for k in metrics_sum.keys():\n",
        "                    metrics_sum[k] += restored_metrics[k]\n",
        "                    delta_sum[k] += delta_metrics[k]\n",
        "\n",
        "                n_batches += 1\n",
        "                val_bar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        val_loss /= n_batches\n",
        "        metrics_avg = {k: v / n_batches for k, v in metrics_sum.items()}\n",
        "        avg_delta = {k: delta_sum[k] / n_batches for k in delta_sum.keys()}\n",
        "\n",
        "        print(f\"\\nΔ delta medio per epoca — \"\n",
        "              f\"MSE:{avg_delta['MSE']:+.5f} | \"\n",
        "              f\"L1:{avg_delta['L1']:+.5f} | \"\n",
        "              f\"COS:{avg_delta['COS']:+.5f} | \"\n",
        "              f\"LSD:{avg_delta['LSD']:+.5f}\")\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_metrics_list.append(metrics_avg)\n",
        "\n",
        "        # Scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        # EARLY STOPPING + CHECKPOINT\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"metrics\": metrics_avg,\n",
        "            }, save_path)\n",
        "            print(f\"Nuovo best model salvato (val_loss={val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Nessun miglioramento ({patience_counter}/{patience})\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping attivato alla epoca {epoch}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        # VISUALIZZAZIONE ESEMPIO\n",
        "        if show_example:\n",
        "            mel_degraded, mel_clean = next(iter(val_dl))\n",
        "            mel_degraded, mel_clean = mel_degraded.to(device), mel_clean.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                enhanced = model(mel_degraded)\n",
        "\n",
        "            idx = 0\n",
        "            plt.figure(figsize=(14, 8))\n",
        "\n",
        "            plt.subplot(3, 1, 1)\n",
        "            im1 = plt.imshow(mel_clean[idx, 0].cpu().numpy(),\n",
        "                            origin='lower', aspect='auto', cmap='viridis')\n",
        "            plt.title(\"Clean (Target)\")\n",
        "            plt.colorbar(im1, fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.subplot(3, 1, 2)\n",
        "            im2 = plt.imshow(mel_degraded[idx, 0].cpu().numpy(),\n",
        "                            origin='lower', aspect='auto', cmap='viridis')\n",
        "            plt.title(\"Degraded (Input)\")\n",
        "            plt.colorbar(im2, fraction=0.046, pad=0.04)\n",
        "\n",
        "            plt.subplot(3, 1, 3)\n",
        "            im3 = plt.imshow(enhanced[idx, 0].cpu().numpy(),\n",
        "                            origin='lower', aspect='auto', cmap='viridis')\n",
        "            plt.title(\"Enhanced (Output)\")\n",
        "            plt.colorbar(im3, fraction=0.046, pad=0.04)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        # LOGGING\n",
        "        if show_metrics:\n",
        "            print(f\"\\nValidation Summary (Epoch {epoch})\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}\")\n",
        "            print(f\"MSE:{metrics_avg['MSE']:.5f} | L1:{metrics_avg['L1']:.5f} | COS:{metrics_avg['COS']:.5f} | LSD:{metrics_avg['LSD']:.5f}\")\n",
        "\n",
        "            if log_loss_components and comp is not None:\n",
        "                print(f\"Loss components --> Total:{comp['total']:.4f} | \"\n",
        "                      f\"charb:{comp['charb']:.4f} | hf:{comp['hf']:.4f} | \"\n",
        "                      f\"bw:{comp['bw']:.4f} | energy:{comp['energy']:.4f} | \"\n",
        "                      f\"dnh:{comp['dnh']:.4f} | flat:{comp['flat']:.4f}\")\n",
        "\n",
        "            print(\"-------------------------------------------\")\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nTraining completato - Best epoch: {best_epoch} | Best val_loss={best_val_loss:.4f}\")\n",
        "    print(f\"\\nCheckpoint salvato in: {save_path}\")\n",
        "    return model, train_losses, val_losses, val_metrics_list\n"
      ],
      "metadata": {
        "id": "yO5dnbzquT43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ATTENZIONE!!!** Se la variabile `skip_training` all'interno del dizionario `CONFIG` è stata impostata a `False` i pesi scaricati dalla repo github verranno sovrascritti."
      ],
      "metadata": {
        "id": "3MEpW5hSXmJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Gruppo A"
      ],
      "metadata": {
        "id": "XAd4JyBYFvgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializziamo il modello\n",
        "model_A = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model_A.parameters(),\n",
        "    lr=1e-4,\n",
        "    betas=(0.9, 0.98), # Controlla il momentum e la media esponenziale dei gradienti\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=5e-6 # Limite inferiore del Learning rate\n",
        ")\n",
        "\n",
        "path_gruppoA = \"/content/projectID2ML/modello_mel/checkpoint_best_gruppoA.pt\"\n",
        "\n",
        "\n",
        "if CONFIG[\"skip_training\"] == True and os.path.exists(path_gruppoA):\n",
        "    checkpoint = torch.load(path_gruppoA, map_location=device)\n",
        "    model_A.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"\\nTraining saltato, caricamento checkpoint da: {path_gruppoA}\")\n",
        "\n",
        "elif CONFIG[\"skip_training\"] == True:\n",
        "    print(f\"\\nRichiesto skip training ma il checkpoint {path_gruppoA} non esiste. Avvio del training.\")\n",
        "    model_A, train_losses_A, val_losses_A, val_metrics_A = train_spectral_model(\n",
        "        model=model_A,\n",
        "        train_dl=train_loader_A,\n",
        "        val_dl=val_loader_A,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoA.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )\n",
        "\n",
        "else:\n",
        "    model_A, train_losses_A, val_losses_A, val_metrics_A = train_spectral_model(\n",
        "        model=model_A,\n",
        "        train_dl=train_loader_A,\n",
        "        val_dl=val_loader_A,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoA.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )"
      ],
      "metadata": {
        "id": "hDn3vNhxuZr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Gruppo B"
      ],
      "metadata": {
        "id": "nfnIlrMYF09a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializziamo il modello\n",
        "model_B = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model_B.parameters(),\n",
        "    lr=1e-4,\n",
        "    betas=(0.9, 0.98), # Controlla il momentum e la media esponenziale dei gradienti\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=5e-6 # Limite inferiore del Learning rate\n",
        ")\n",
        "\n",
        "path_gruppoB = \"/content/projectID2ML/modello_mel/checkpoint_best_gruppoB.pt\"\n",
        "\n",
        "\n",
        "if CONFIG[\"skip_training\"] == True and os.path.exists(path_gruppoB):\n",
        "    checkpoint = torch.load(path_gruppoB, map_location=device)\n",
        "    model_B.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    print(f\"\\nTraining saltato, caricamento checkpoint da: {path_gruppoB}\")\n",
        "\n",
        "elif CONFIG[\"skip_training\"] == True:\n",
        "    print(f\"\\nRichiesto skip training ma il checkpoint {path_gruppoB} non esiste. Avvio del training.\")\n",
        "\n",
        "    model_B, train_losses_B, val_losses_B, val_metrics_B = train_spectral_model(\n",
        "        model=model_B,\n",
        "        train_dl=train_loader_B,\n",
        "        val_dl=val_loader_B,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoB.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )\n",
        "\n",
        "else:\n",
        "    model_B, train_losses_B, val_losses_B, val_metrics_B = train_spectral_model(\n",
        "        model=model_B,\n",
        "        train_dl=train_loader_B,\n",
        "        val_dl=val_loader_B,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=25,\n",
        "        patience=5,\n",
        "        device=device,\n",
        "        save_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoB.pt\",\n",
        "        show_example=True, # Visualizza uno spettrogramma per epoca\n",
        "        show_metrics=True, # Mostra le metriche\n",
        "        use_amp=True, # Mixed precision\n",
        "        log_loss_components=True # Stampa breakdown della loss nel validation\n",
        "    )"
      ],
      "metadata": {
        "id": "ICSB5GQ_uexj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "fJm69g_PF5Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante la fase di Test e valutazione finale dobbiamo misurare quanto bene il modello ha ricostruito gli spettrogrammi.\n",
        "\n"
      ],
      "metadata": {
        "id": "B_p7lzGa8_5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop"
      ],
      "metadata": {
        "id": "FzF2gThmF7YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funzione `test_spectral_model` eseguiamo la valutazione finale del modello utilizzando il set di test.\n",
        "\n",
        "All'inizio carichiamo il miglior checkpoint salvato nel training. Impostiamo il modello in modalità `eval`, inizializziamo un `GradScaler` e i dizionari che conterranno i valori dele metriche.\n",
        "\n",
        "Per ogni batch:\n",
        "\n",
        "1. calcoliamo lo spettrogramma `enhanced`;\n",
        "2. allineiamo le dimensioni con `crop_to_match`;\n",
        "3. normalizziamo i tensori;\n",
        "4. calcoliamo la loss;\n",
        "5. calcoliamo le metriche;\n",
        "6. confrontiamo le metriche di `degraded` ed `enhanced` calcolando il delta (Δ) per ogni metrica.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bwi-jTQP-57r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_spectral_model(\n",
        "    model,\n",
        "    test_dl,\n",
        "    loss_fn,\n",
        "    checkpoint_path=\"/content/checkpoint_best.pt\",\n",
        "    device=\"cuda\",\n",
        "    use_amp=True,\n",
        "    log_loss_components=False,\n",
        "    show_example=True\n",
        "):\n",
        "\n",
        "\n",
        "    # Caricamento checkpoint\n",
        "    print(f\"\\n Caricamento checkpoint da: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(f\" Checkpoint caricato (epoch {checkpoint['epoch']} — val_loss={checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "\n",
        "    # Inizializza metriche\n",
        "    total_loss = 0.0\n",
        "    metrics_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0, \"LSD\": 0}\n",
        "    baseline_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0, \"LSD\": 0}\n",
        "    delta_sum = {\"MSE\": 0, \"L1\": 0, \"COS\": 0, \"LSD\": 0}\n",
        "    n_batches = 0\n",
        "\n",
        "    scaler = amp.GradScaler('cuda', enabled=use_amp)\n",
        "    print(\"\\n Inizio test su set di test...\")\n",
        "    test_bar = tqdm(test_dl, desc=\"Testing\", leave=False)\n",
        "\n",
        "\n",
        "    # Loop di test\n",
        "    with torch.no_grad():\n",
        "        for mel_degraded, mel_clean in test_bar:\n",
        "            mel_degraded = mel_degraded.to(device, non_blocking=True)\n",
        "            mel_clean = mel_clean.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward\n",
        "            with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                enhanced = model(mel_degraded)\n",
        "                enhanced, mel_clean = crop_to_match(enhanced, mel_clean)\n",
        "                mel_degraded, _ = crop_to_match(mel_degraded, mel_clean)\n",
        "\n",
        "                # Normalizzazione\n",
        "                ref = mel_clean.amax()\n",
        "                ref = torch.clamp(ref, min=1e-8)\n",
        "                mel_clean_n = mel_clean / ref\n",
        "                mel_degraded_n = mel_degraded / ref\n",
        "                enhanced_n = enhanced / ref\n",
        "\n",
        "                # Loss\n",
        "                if log_loss_components:\n",
        "                    out = loss_fn(enhanced_n, mel_clean_n, degraded=mel_degraded_n, return_components=True)\n",
        "                    loss = out[\"total\"]\n",
        "                    comp = {k: v.item() if torch.is_tensor(v) else v for k, v in out.items()}\n",
        "                else:\n",
        "                    loss = loss_fn(enhanced_n, mel_clean_n, mel_degraded_n)\n",
        "                    comp = None\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Metriche enhanced vs clean\n",
        "            restored_metrics = {\n",
        "                \"MSE\": mse_spec(enhanced.cpu(), mel_clean.cpu()),\n",
        "                \"L1\":  l1_spec(enhanced.cpu(), mel_clean.cpu()),\n",
        "                \"COS\": cosine_spec(enhanced.cpu(), mel_clean.cpu()),\n",
        "                \"LSD\": lsd_spec(enhanced.cpu(), mel_clean.cpu(), CONFIG[\"top_db\"])\n",
        "            }\n",
        "\n",
        "            # Baseline (degraded vs clean)\n",
        "            baseline_metrics = {\n",
        "                \"MSE\": mse_spec(mel_degraded.cpu(), mel_clean.cpu()),\n",
        "                \"L1\":  l1_spec(mel_degraded.cpu(), mel_clean.cpu()),\n",
        "                \"COS\": cosine_spec(mel_degraded.cpu(), mel_clean.cpu()),\n",
        "                \"LSD\": lsd_spec(mel_degraded.cpu(), mel_clean.cpu(), CONFIG[\"top_db\"])\n",
        "            }\n",
        "\n",
        "            # Δ baseline - restored\n",
        "            delta_metrics = {k: baseline_metrics[k] - restored_metrics[k] for k in restored_metrics}\n",
        "\n",
        "            for k in metrics_sum.keys():\n",
        "                metrics_sum[k] += restored_metrics[k]\n",
        "                baseline_sum[k] += baseline_metrics[k]\n",
        "                delta_sum[k] += delta_metrics[k]\n",
        "\n",
        "            n_batches += 1\n",
        "            test_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "\n",
        "    # Calcolo medie e report\n",
        "    avg_loss = total_loss / n_batches\n",
        "    avg_restored = {k: metrics_sum[k] / n_batches for k in metrics_sum}\n",
        "    avg_baseline = {k: baseline_sum[k] / n_batches for k in baseline_sum}\n",
        "    avg_delta = {k: delta_sum[k] / n_batches for k in delta_sum}\n",
        "\n",
        "    print(\"\\nRISULTATI MEDI SU TEST SET:\")\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "    for k in avg_restored.keys():\n",
        "        print(f\"{k}: base={avg_baseline[k]:.5f} | restored={avg_restored[k]:.5f} | Δ={avg_delta[k]:+.5f}\")\n",
        "\n",
        "    if log_loss_components and comp is not None:\n",
        "        print(\"\\nUltime componenti di loss viste:\")\n",
        "        print(\" | \".join(f\"{k}:{v:.4f}\" for k, v in comp.items() if k != \"total\"))\n",
        "        print(f\"Total: {comp['total']:.4f}\")\n",
        "\n",
        "\n",
        "    # Visualizzazione esempio\n",
        "    if show_example:\n",
        "        mel_degraded, mel_clean = next(iter(test_dl))\n",
        "        mel_degraded, mel_clean = mel_degraded.to(device), mel_clean.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enhanced = model(mel_degraded)\n",
        "\n",
        "        idx = 0\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        plt.subplot(3, 1, 1)\n",
        "        im1 = plt.imshow(mel_clean[idx, 0].cpu().numpy(), origin='lower', aspect='auto', cmap='viridis')\n",
        "        plt.title(\"Clean (Target)\")\n",
        "        plt.colorbar(im1, fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.subplot(3, 1, 2)\n",
        "        im2 = plt.imshow(mel_degraded[idx, 0].cpu().numpy(), origin='lower', aspect='auto', cmap='viridis')\n",
        "        plt.title(\"Degraded (Input)\")\n",
        "        plt.colorbar(im2, fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.subplot(3, 1, 3)\n",
        "        im3 = plt.imshow(enhanced[idx, 0].cpu().numpy(), origin='lower', aspect='auto', cmap='viridis')\n",
        "        plt.title(\"Enhanced (Output)\")\n",
        "        plt.colorbar(im3, fraction=0.046, pad=0.04)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return avg_baseline, avg_restored, avg_delta\n"
      ],
      "metadata": {
        "id": "qDzoj5v7vA8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop su Modello A"
      ],
      "metadata": {
        "id": "4D8WqpeaGD_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo A"
      ],
      "metadata": {
        "id": "s0vD2vyxGJsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_A = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_AA, restored_AA, delta_AA = test_spectral_model(\n",
        "    model=model_A,\n",
        "    test_dl=test_loader_A,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoA.pt\",\n",
        "    device=\"cuda\",\n",
        "    use_amp=True,\n",
        "    log_loss_components=False,\n",
        "    show_example=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "C_3NHBFpvENE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo B"
      ],
      "metadata": {
        "id": "1z4FfUTKGN7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_A = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_AB, restored_AB, delta_AB = test_spectral_model(\n",
        "    model=model_A,\n",
        "    test_dl=test_loader_B,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoA.pt\",\n",
        "    device=\"cuda\",\n",
        "    use_amp=True,\n",
        "    log_loss_components=False,\n",
        "    show_example=True\n",
        ")"
      ],
      "metadata": {
        "id": "_K-JTpYYvJKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Loop su Modello B"
      ],
      "metadata": {
        "id": "69Dd-_AEGQsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo A"
      ],
      "metadata": {
        "id": "TG3L3fiOGQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_B = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_BA, restored_BA, delta_BA = test_spectral_model(\n",
        "    model=model_B,\n",
        "    test_dl=test_loader_A,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoB.pt\",\n",
        "    device=\"cuda\",\n",
        "    use_amp=True,\n",
        "    log_loss_components=False,\n",
        "    show_example=True\n",
        ")"
      ],
      "metadata": {
        "id": "P_jouIJnvWI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Degrdazioni Gruppo B"
      ],
      "metadata": {
        "id": "S2BmxlpMGQsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_B = SpectralEnhancementNet(n_mels=CONFIG[\"n_mels\"]).to(device)\n",
        "\n",
        "loss_fn = HybridLoss()\n",
        "\n",
        "baseline_BB, restored_BB, delta_BB = test_spectral_model(\n",
        "    model=model_B,\n",
        "    test_dl=test_loader_B,\n",
        "    loss_fn=loss_fn,\n",
        "    checkpoint_path=\"/content/projectID2ML/modello_mel/checkpoint_best_gruppoB.pt\",\n",
        "    device=\"cuda\",\n",
        "    use_amp=True,\n",
        "    log_loss_components=False,\n",
        "    show_example=True\n",
        ")"
      ],
      "metadata": {
        "id": "YA0pxf5AvTh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi dei Risultati"
      ],
      "metadata": {
        "id": "wDTza6H8GZXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ho addestrato il modello `Spectral Enhancement` su due domini diversi, ciascuno dedicato a una diversa famiglia di degradazioni spettrali.\n",
        "Il primo modello è stato addestrato sul **gruppo A** (quantize, tonal stripes, noise), caratterizzato da degradazioni additive o periodiche, mentre il secondo sul **gruppo B** (clipping, reverb, lowpass, distort), che comprende distorsioni non lineari o di tipo convolutivo."
      ],
      "metadata": {
        "id": "NRO6BkRYsJ92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modello Addestrato sul gruppo A**\n",
        "\n",
        "Il modello addestrato sul gruppo A mostra una rapida convergenza già nelle prime epoche.\n",
        "\n",
        "\n",
        "Un aspetto interessante è che, in tutte le epoche, la validation loss risulta inferiore alla training loss.\n",
        "Questo comportamento, apparentemente anomalo, può essere spiegato da diversi fattori:\n",
        "\n",
        "* Dropout o altre forme di regolarizzazione attive in fase di training, che aumentano artificialmente la perdita media sui batch di addestramento.\n",
        "\n",
        "* Batch norm e mixed precision, che rendono più stabile la distribuzione dei dati in validazione rispetto a quella del training.\n",
        "\n",
        "* Dataset di validazione più \"semplice\", cioè con esempi mediamente meno rumorosi o più regolari rispetto al dataset di training.\n",
        "\n",
        "* L’uso di early stopping e scheduler adattivo può favorire una riduzione più rapida della loss di validazione rispetto a quella di training.\n",
        "\n",
        "Nel complesso, il modello A apprende le caratteristiche spettrali tipiche delle degradazioni additive, riuscendo a ripristinare strutture armoniche e a ridurre il rumore ad alta frequenza.\n",
        "\n",
        "Il test sullo stesso dominio (*gruppo A*) conferma questa capacità di generalizzazione in-domain,\n",
        "le metriche mostrano un miglioramento leggero di MSE e LSD, mentre la somiglianza coseno (COS) aumenta lievemente, segno che la coerenza armonica è stata preservata.\n",
        "L’unica metrica con leggera flessione è L1, dovuta al fatto che il modello tende a levigare alcune componenti spettrali residue, riducendo la granularità fine del segnale ma mantenendo la sua forma complessiva.\n",
        "\n",
        "Nel complesso, il modello A si comporta come previsto, migliora la qualità spettrale, riduce il rumore e ricostruisce le armoniche principali, con una piccola perdita di dettaglio dovuta all’azione regolarizzante della loss.\n",
        "\n",
        "Il test out-of-domain (cioè testando il modello A sul *gruppo B*) mostra invece un netto calo prestazionale.\n",
        "Il modello, addestrato su pattern additivi e stazionari, non riesce a gestire degradazioni non lineari o a banda limitata, come clipping e riverbero.\n",
        "Di conseguenza, l’output enhanced risulta in media peggiore del segnale degradato, con un aumento dell’errore e una riduzione della coerenza spettrale.\n",
        "Questo comportamento è atteso, la rete ha imparato rappresentazioni molto specifiche per il tipo di rumore del gruppo A, ma non ha sviluppato una rappresentazione sufficientemente astratta e generalizzabile per adattarsi a distorsioni di natura diversa."
      ],
      "metadata": {
        "id": "5tCG3HoHslM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modello Addestrato sul gruppo B**\n",
        "\n",
        "Anche il modello addestrato sul gruppo B mostra una rapida convergenza.\n",
        "\n",
        "\n",
        "Anche in questo caso, la validation loss è spesso inferiore alla train loss.\n",
        "Le cause possono essere analoghe a quelle del modello A.\n",
        "\n",
        "\n",
        "Il test in-domain (sul *gruppo B*) conferma un miglioramento in tutte le metriche,\n",
        "il modello riesce a ridurre l’MSE e la LSD, migliorando la chiarezza e la distribuzione armonica dei segnali ricostruiti.\n",
        "\n",
        "\n",
        "Il test out-of-domain (modello B testato sul *gruppo A*) mostra invece un comportamento speculare al caso precedente, il modello fallisce nel generalizzare su degradazioni additive o tonali, restituendo prestazioni inferiori al segnale degradato originale.\n",
        "\n",
        "Questo conferma che ciascun modello ha appreso pattern molto specifici del proprio dominio, ma manca di robustezza cross domain."
      ],
      "metadata": {
        "id": "JDO2c1BNtuiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possibili Miglioramenti**\n",
        "\n",
        "Per migliorare le prestazioni e ottenere un modello che generalizza meglio, si possono considerare diverse strategie:\n",
        "\n",
        "* **Addestramento multi-dominio** (A + B): combinare i due set di degradazioni in un unico dataset, in modo che la rete apprenda una rappresentazione condivisa delle diverse tipologie di distorsione.\n",
        "\n",
        "* **Continual Learning**: per migliorare la generalizzazione tra domini si potrebbero implementare tecniche di apprendimento continuo, in cui il modello viene addestrato progressivamente su differenti tipi di degradazioni (prima il gruppo A, poi il gruppo B) senza dimenticare le conoscenze apprese in precedenza (ad esempio con regolarizzazione dei pesi o replay selettivo). Ciò permetterebbe di ottenere un modello più flessibile e generalizzabile, capace di adattarsi a nuovi tipi di distorsione nel tempo.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6IW40DBLvC1J"
      }
    }
  ]
}